import os
import sys
import pandas as pd
import pickle
from vectorizer_loader import load_tfidf_vectorizer
from tweet_vectorization import tweet_vector_tfidf

# Sauvegarder l'état initial de sys.path
original_sys_path = sys.path.copy()
# Ajouter le répertoire parent au chemin pour l'import
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from params_utils.params_utils import load_params  
from logging_utils.logging_utils import get_logger
# Restaurer l'état initial de sys.path
sys.path = original_sys_path

# Obtenir le logger
logger = get_logger(__name__)

from scipy.sparse import vstack
import pandas as pd
import pickle
import os

def apply_tfidf_vectorizer(input_file, vectorizer, output_file, text_column, label_column):
    """
    Applique le vectorizer TF-IDF à un fichier d'entrée et sauvegarde les résultats.

    Args:
        input_file (str): Chemin vers le fichier d'entrée CSV.
        vectorizer (TfidfVectorizer): Instance du vectorizer TF-IDF chargé.
        output_file (str): Chemin pour sauvegarder le fichier vectorisé.
        text_column (str): Nom de la colonne contenant les textes.
        label_column (str): Nom de la colonne contenant les labels ou IDs.

    Returns:
        None
    """
    logger.info(f"Chargement des données depuis : {input_file}")
    data = pd.read_csv(input_file)

    # Vérifier que les colonnes existent
    if text_column not in data.columns:
        raise ValueError(f"La colonne '{text_column}' est absente dans {input_file}.")
    if label_column not in data.columns:
        raise ValueError(f"La colonne '{label_column}' est absente dans {input_file}.")

    # Supprimer les tweets vides
    initial_count = len(data)
    data = data[data[text_column].notna() & data[text_column].str.strip().astype(bool)]
    cleaned_count = len(data)

    logger.info(f"Nettoyage des données terminé : {initial_count - cleaned_count} tweets vides supprimés.")

    # Appliquer `tweet_vector_tfidf` pour chaque tweet individuellement
    vectors = []
    ids = []

    for index, row in data.iterrows():
        tweet = row[text_column]
        label = row[label_column]

        try:
            # Vectoriser le tweet
            vector = tweet_vector_tfidf(tweet, vectorizer)
            vectors.append(vector)
            ids.append(label)
        except Exception as e:
            logger.warning(f"Erreur lors de la vectorisation du tweet (index {index}) : {e}")

    # Convertir les vecteurs en matrice sparse
    vectors = vstack(vectors)

    # Préparer les données de sortie
    output_data = {
        "vectors": vectors,
        "ids": ids
    }

    # Sauvegarder dans un fichier pickle
    logger.info(f"Sauvegarde des données vectorisées dans : {output_file}")
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, "wb") as f:
        pickle.dump(output_data, f)

    logger.info("Vectorisation TF-IDF terminée.")

# -------------------------
# Point d'entrée principal
# -------------------------
if __name__ == "__main__":
    # Charger les paramètres
    params_file = "params.yaml"
    section = "batch_apply_tfidf_vectorizer"

    try:
        params = load_params(params_file, section)
    except KeyError as e:
        logger.error(f"Erreur dans le fichier des paramètres : {e}")
        raise

    # Chemins des fichiers
    vectorizer_file = params["vectorizer_file"]
    input_files = params["input"]
    output_dir = params["output_dir"]
    text_column = params["text_column"]
    label_column = params["label_column"]

    # Charger le vectorizer TF-IDF
    logger.info(f"Chargement du vectorizer TF-IDF depuis : {vectorizer_file}")
    vectorizer = load_tfidf_vectorizer(vectorizer_file)

    # Appliquer le vectorizer à chaque fichier
    for split_name, input_file in input_files.items():
        logger.info(f"Traitement du fichier : {input_file}")
        output_file = os.path.join(output_dir, f"{split_name}_tfidf.pkl")
        apply_tfidf_vectorizer(input_file, vectorizer, output_file, text_column, label_column)
