{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps : Quand le Machine Learning rencontre l’esprit du DevOps\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Les projets de Machine Learning (ML) sont souvent comparés à des expériences scientifiques : on explore, on teste, on ajuste. Une sorte de laboratoire où chaque idée est mise à l’épreuve. Mais cette approche, bien qu’efficace pour apprendre, montre vite ses limites quand il s’agit de passer à la production.\n",
    "\n",
    "Avec des modèles plus complexes, plus de données, et des besoins de monitoring, il ne suffit plus que « ça marche dans le notebook ». C’est ici que le MLOps, inspiré du DevOps, entre en jeu : il structure, automatise et industrialise chaque étape d’un projet ML. Pour le démontrer, prenons un exemple concret avec un **projet d’analyse de sentiments**. Il s'agit de créer un prototype d'IA pour prédire le sentiment des tweets à partir de données open source et déployé le model via une API Cloud.\n",
    "\n",
    "## 1) Premier pas\n",
    "\n",
    "L'une des premières étapes est le choix du modèle, c'est une étape essentielle mais souvent complexe. Cela dépend des spécificités des données, des contraintes du projet et des objectifs visés. Une approche pragmatique consiste à commencer par un modèle simple, puis à explorer des modèles de plus en plus avancés.\n",
    "\n",
    "Pour notre projet d'analyse de sentiment, nous commençons par **une régression logistique**. Pourquoi ? Parce qu’elle est rapide, facile à implémenter et souvent suffisante pour valider une idée. Avec cette méthode, nous avons utilisé des caractéristiques textuelles de base, comme la fréquence des mots (bag of words). Cette **baseline** joue un double rôle : elle permet de valider rapidement la faisabilité de l'approche et sert de référence pour évaluer les améliorations apportées par des modèles plus complexes.\n",
    "\n",
    "Une fois que notre régression logistique est en place et que nous avons une baseline, il devient crucial de structurer notre approche pour garantir la reproductibilité des résultats, faciliter la comparaison des expérimentations, et préparer le modèle au déploiement en production, ce que permettent les principes du MLOps et des outils comme **MLflow**. **MLflow** stocke les métadonnées de run et les modèles, tout en assurant le suivi des expérimentations.\n",
    "\n",
    "\n",
    "\n",
    "En ce qui concerne MLops, nous allons utiliser conjointement plusieurs outils :\n",
    "- **MLflow**: stocke les métadonnées de run et les modèles.\n",
    "- **DVC**: stocke et versionne les datasets, sans multiplier l’espace à chaque run. DVC est conçu pour gérer efficacement les changements et éviter de télécharger ou de pousser inutilement des données si elles n'ont pas été modifiées.\n",
    "- **Git**: stocke les scripts, configurations et fichiers .dvc nécessaires pour synchroniser code et données, assurant une traçabilité complète du projet.\n",
    "- **Docker/Conda**: Garantie d'avoir un environnement reproductible. \n",
    "    - ex : conda pack -n mlops-env -o mlops-env.tar.gz && docker build -f Dockerfile.mlops-env -t mlops-env:v1 .\n",
    "\n",
    "Ensemble, ces outils créent un écosystème intégré qui garantit la reproductibilité des résultats en synchronisant chaque version du code, des données et des modèles. Ils facilitent également la collaboration entre équipes en offrant un suivi clair des expérimentations et des changements, tout en structurant des pipelines de Machine Learning robustes et facilement déployables à l'échelle.\n",
    "\n",
    "\n",
    "\n",
    "### Points clés :\n",
    "- Utilisation de Docker\n",
    "- L'utilisation de DVC et git permet de faire en sorte que le script \n",
    "- l'utilisation d'Hydra permet de changer de passer d'une expérience à autre simplement.\n",
    "\n",
    "### Résultats :\n",
    "Voici les performances obtenues sur notre dataset :\n",
    "- Précision : 76,5%\n",
    "- F1-score : 74,2%\n",
    "\n",
    "## Création du pipeline avec DVC (Data Version Control)\n",
    "\n",
    "Dans un pipeline DevOps classique, vous codez, vous poussez sur Git, et boum, tout se construit et se déploie automatiquement. En MLOps, on va un cran plus loin, les étapes classiques sont : \n",
    "- Préparer les données (nettoyage, enrichissement).\n",
    "- Entraîner les modèles (et tester plusieurs configurations).\n",
    "- Valider les résultats (cross-validation, ensembles de test).\n",
    "- Et ensuite ? Surveiller en continu. Parce qu’un modèle, contrairement à du code, peut se dégrader avec le temps !(drift)\n",
    "\n",
    "En machine learning, les pipelines impliquent non seulement du code, mais aussi des données, des modèles, et des étapes de transformation interconnectées. C'est là que DVC devient un outil indispensable. Inspiré de Git, DVC détecte automatiquement les changements dans votre pipeline, ne réexécutant que les étapes nécessaires pour garantir une gestion efficace et reproductible des workflows ML.\n",
    "\n",
    "\n",
    "## Monter en puissance avec des modèles avancés\n",
    "\n",
    "Bien que ce soit un bon point de départ, le modèle manque de finesse pour capturer des relations contextuelles. Pour améliorer les résultats, nous avons besoin de tester des modèles plus avancés, comme des word embeddings combinés à des réseaux neuronaux (CNN ou RNN). Ces approches permettent de mieux comprendre le contexte des mots en tenant compte des séquences.\n",
    "\n",
    "Avec Hydra, il devient facile d'ajouter des configurations sans trop toucher aux scripts python.\n",
    "\n",
    "Capture d’écran des résultats et du graphe des erreurs communes.\n",
    "\n",
    "Résultat avec un CNN :\n",
    "- Précision : 82,7%\n",
    "- F1-score : 81,4%\n",
    "\n",
    "Cette amélioration montre que le modèle capture désormais des nuances plus subtiles dans les sentiments. Cependant, la complexité augmente : le modèle prend plus de temps à entraîner, et le suivi des hyperparamètres devient critique.\n",
    "\n",
    "Capture d’écran des résultats comparés aux erreurs de la régression logistique.\n",
    "\n",
    "## Étape 3 : L’État de l’art avec BERT\n",
    "\n",
    "Enfin, nous avons utilisé BERT, un modèle préentraîné d’État de l’art, capable de comprendre les subtilités linguistiques et les relations complexes dans les phrases. Avec BERT, les résultats ont été impressionnants :\n",
    "\n",
    "Résultat avec BERT :\n",
    "- Précision : 89,5%\n",
    "- F1-score : 88,9%\n",
    "\n",
    "Cependant, l’utilisation de BERT a introduit de nouveaux défis :\n",
    "- Temps d’entraînement prolongé : Il a fallu 10 fois plus de temps que pour la régression logistique.\n",
    "- Infrastructure nécessaire : L’entraînement a nécessité un GPU, et le suivi des versions de données et de modèles est devenu encore plus crucial.\n",
    "\n",
    "Capture d’écran des résultats, avec une visualisation des prédictions BERT sur des exemples complexes.\n",
    "\n",
    "## Passage en production avec MLOps\n",
    "\n",
    "À chaque étape, les modèles deviennent plus performants, mais aussi plus complexes à gérer. C’est ici que le MLOps prend tout son sens. Voici comment il a structuré notre projet :\n",
    "\n",
    "### 1. Suivi des expérimentations\n",
    "\n",
    "Grâce à des outils comme MLflow, nous avons enregistré chaque expérimentation :\n",
    "- Version des données utilisées.\n",
    "- Hyperparamètres et configurations des modèles.\n",
    "- Performances obtenues (précision, F1-score, etc.).\n",
    "\n",
    "Cela nous a permis de comparer les résultats facilement et de justifier nos choix à chaque étape.\n",
    "\n",
    "### 2. Automatisation des pipelines\n",
    "\n",
    "Nous avons automatisé les étapes clés :\n",
    "- Préparation des données (nettoyage, transformation).\n",
    "- Entraînement des modèles avec différentes configurations.\n",
    "- Validation sur des ensembles indépendants.\n",
    "\n",
    "Un outil comme Kubeflow nous a aidés à orchestrer ces tâches et à les rendre reproductibles.\n",
    "\n",
    "### 3. Monitoring en production\n",
    "\n",
    "Une fois BERT déployé, un système de monitoring a été mis en place pour surveiller les performances en temps réel. Par exemple :\n",
    "- Détection des drifts de données : Si les avis clients évoluent (argot, emojis, etc.).\n",
    "- Suivi des métriques : Si le modèle perd en précision ou en rappel.\n",
    "\n",
    "Conclusion : Apprendre, structurer, et industrialiser\n",
    "\n",
    "L’histoire de ce projet d’analyse de sentiments montre bien la double nature des projets ML. D’un côté, il y a l’aspect exploratoire : tester différents modèles et approches pour trouver ce qui fonctionne. De l’autre, il y a le besoin d’industrialisation, où la reproductibilité et la fiabilité deviennent essentielles.\n",
    "\n",
    "Le MLOps, en s’appuyant sur les principes du DevOps, nous a permis de structurer cette transition. Il a transformé une série d’expérimentations en une solution robuste, prête pour la production. Et à chaque étape, il nous a offert des outils pour gérer la complexité croissante du Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quand le Machine Learning rencontre la philosophie DevOps\n",
    "\n",
    "(# Possibilité d’ajouter un mini-schéma illustrant le concept général de MLOps : données -> entraînement -> déploiement -> monitoring.)\n",
    "\n",
    "Un projet de Machine Learning (ML) est comparable à une suite d'expériences scientifiques : on explore, on teste, on ajuste. Une sorte de laboratoire où chaque idée est mise à l’épreuve. Mais cette approche, bien qu’efficace au début, montre vite ses limites quand il s’agit de passer à la production. Avec des modèles de plus en plus complexes, il ne suffit plus que « ça marche dans le notebook ». Les tests isolés et les configurations manuelles deviennent vite des goulets d'étranglement, rendant difficile la collaboration entre les équipes, la traçabilité des versions de modèles, ou encore leur déploiement à grande échelle. C’est ici que le MLOps entre en jeu. Inspiré des pratiques du DevOps, le MLOps propose une approche systématique pour structurer, automatiser et industrialiser chaque étape du cycle de vie d’un projet ML, de l’acquisition des données jusqu’au suivi des modèles en production.\n",
    "\n",
    "Pour illustrer les avantages concrets du MLOps, nous prendrons l’exemple d’une analyse de sentiments basée sur des tweets. Ce projet servira de fil conducteur pour montrer comment des outils tels que MLflow, DVC, Hydra, et Docker/Kubernetes peuvent répondre aux défis techniques et organisationnels rencontrés tout au long du cycle de vie d’un projet ML : de la gestion des données à la mise en production. Nous verrons également comment le MLOps s’affirme comme un levier stratégique essentiel pour assurer la pérennité, la scalabilité et l’impact des solutions d’intelligence artificielle. Au-delà de la méthode, il s’agit d’une discipline clé, dont les compétences sont distinctes mais complémentaires de celles d’un Data Scientist, et indispensables pour déployer les projets d’IA à grande échelle.\n",
    "\n",
    "## MLflow : Le carnet de bord des expérimentations ML\n",
    "\n",
    "(# Ici, vous pouvez insérer une capture d'écran MLflow ou un court code montrant l'utilisation de mlflow.start_run(), mlflow.log_param(), mlflow.log_metric()...)\n",
    "\n",
    "L'une des premières étapes dans un projet ML est le choix du modèle. Cette étape essentielle est souvent complexe et dépend autant des spécificités des données que des contraintes du projet et des objectifs visés. Pour démarrer, il est judicieux de commencer par un modèle simple, puis d’explorer des modèles plus avancés. Dans notre cas, nous avons opté pour une régression logistique. Pourquoi ? Parce qu’elle est rapide, facile à implémenter et dsouvent suffisante pour valider la faisabilité de l’approche. Ce modèle, que l’on nomme baseline, joue un double rôle :\n",
    "- Confirmer rapidement la validité de la démarche.\n",
    "- Servir de référence pour évaluer les gains apportés par des modèles plus sophistiqués.\n",
    "\n",
    "### Capturer les métriques\n",
    "\n",
    "Dans le cadre de ce projet d’analyse de sentiments, la baseline (régression logistique) est enregistrée dans le Model Registry de MLflow, ce qui offre un suivi centralisé de ses performances.\n",
    "Comparaison de la baseline avec un modèle plus avancé\n",
    "\n",
    "Lorsque de nouveaux modèles sont testés (par exemple un réseau de neurones), MLflow facilite la comparaison avec la baseline, tant au niveau des métriques que des hyperparamètres utilisés. Un simple coup d’œil à l’interface permet de voir si le nouveau modèle surpasse ou non la baseline initiale.\n",
    "\n",
    "##  Un pipeline unique ou plusieurs ?\n",
    "\n",
    "(# Rajouter éventuellement un bref aperçu YAML/Hydra montrant comment vous paramétrez le vectoriseur et le modèle.)\n",
    "\n",
    "Cependant, la baseline n’est qu’un point de départ : à mesure que l’on itère, que l’on ajuste les hyperparamètres et que l’on évalue différents modèles, il devient primordial de garder une traçabilité précise de chaque expérimentation. Dans notre projet, nous souhaitions par exemple tester plusieurs vectoriseurs (TF-IDF, GloVe, BERT…), différents modèles (régression logistique, LSTM, Random Forest, etc.) ou encore des configurations variées (batch_size, lr, seuils de découpage des données, etc.).\n",
    "\n",
    "Pour éviter de multiplier les scripts et de rendre la maintenance laborieuse, on s’appuie sur un pipeline modulaire et paramétrable. Naïvement, deux approches sont possibles :\n",
    "- Dupliquer le pipeline pour chaque variante,\n",
    "- Créer un pipeline modulaire et paramétrable, où la vectorisation, le modèle ou les hyperparamètres sont gérés via un système de configuration (ex. Hydra, YAML, etc.).\n",
    "\n",
    "La première approche peut paraître simple mais devient vite coûteuse en maintenance et en risques de divergence de code (copies multiples). Le guide Google “Continuous delivery and automation pipelines in machine learning” (souvent cité comme une référence MLOps) préconise ainsi :\n",
    "\n",
    "    “To reduce complexity and promote reuse, we recommend building modular and parametric pipelines. Each stage (data processing, training, evaluation) is defined once and can be reused by multiple variants or parameter sets. This avoids duplication of code and simplifies maintenance.”\n",
    "\n",
    "En d’autres termes, un pipeline modulaire et paramétrable reste plus souple, plus facile à tester et à maintenir, tout en assurant la traçabilité au sein d’une même structure de code. Grâce à une configuration centralisée, vous n’avez qu’à adapter quelques paramètres pour changer de vectoriseur ou de modèle, ce qui accélère considérablement l’expérimentation et limite les erreurs.\n",
    "\n",
    "### CI/CD Github\n",
    "(# Rajoutez une courte phrase expliquant ce qu’on voit dans la capture d’écran, par ex. “Ci-dessous un exemple de workflow GitHub Actions déclenché à chaque push, qui exécute nos tests et notre pipeline MLOps.”)\n",
    "Capture d'écran\n",
    "\n",
    "## Passage en production\n",
    "\n",
    "(# Ici, vous pouvez mentionner comment vous packagez le modèle, par exemple via Docker. Ou comment vous utilisez un orchestrateur type Kubernetes, ou un service. Expliquer très brièvement la démarche.)\n",
    "\n",
    "Après avoir validé un modèle satisfaisant, il faut le déployer pour qu’il soit accessible en production. Cette étape peut impliquer :\n",
    "- La containerisation (via Docker) pour embarquer le modèle et son environnement d’exécution,\n",
    "- Le déploiement sur un cluster Kubernetes ou un service d’hébergement (AWS, GCP, Azure…),\n",
    "- La mise en place d’une API (REST, gRPC) permettant de recevoir des données et de retourner des prédictions,\n",
    "- Des scripts ou workflows CI/CD (GitHub Actions, GitLab CI, Jenkins…) assurant que chaque nouvelle version du modèle est correctement testée et livrée.\n",
    "\n",
    "(# Ajouter un exemple rapide : “Nous avons créé un Dockerfile décrivant l’environnement, qui inclut Python 3.X, scikit-learn, MLflow, etc. Ensuite, un script de déploiement…”)\n",
    "DVC et MLflow : Un duo pour une gestion cohérente des modèles et des données\n",
    "\n",
    "Après avoir défini une baseline et exploré des modèles plus avancés, la gestion des données et des versions devient essentielle. Il ne suffit pas de versionner les modèles avec MLflow ou les données avec DVC, mais de garantir un lien explicite entre les deux. Cette association assure que chaque modification repose sur un environnement traçable et reproductible, une pierre angulaire du MLOps.\n",
    "\n",
    "Prenons un exemple concret : reprendre un modèle tagué en préproduction dans MLflow, restaurer son environnement exact, et créer une nouvelle branche Git pour préparer une mise à jour.\n",
    "```bash\n",
    "# Récupérer le hash Git depuis MLflow\n",
    "mlflow models describe -m \"models:/sentiment-analysis-baseline/Production\" > model_metadata.txt\n",
    "GIT_COMMIT=$(grep \"git_commit\" model_metadata.txt | awk -F': ' '{print $2}')\n",
    "\n",
    "# Restaurer l'environnement\n",
    "git checkout $GIT_COMMIT\n",
    "dvc checkout\n",
    "dvc pull\n",
    "```\n",
    "\n",
    "(# Expliquer en quelques mots ce qui se passe dans le script. “mlflow models describe...” récupère le commit Git associé au modèle, on fait ensuite git checkout pour retrouver le code, dvc checkout et dvc pull pour synchroniser les données.”)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "(# Vous pouvez illustrer comment le monitoring ou la dérive de données peut être détecté, ou simplement raccourcir à votre convenance.)\n",
    "\n",
    "En définitive, l’approche MLOps va bien au-delà du simple « ça marche dans le notebook ». En outillant chaque étape (tracking des expériences, versioning des données, CI/CD et déploiement), elle garantit traçabilité, collaboration et industrialisation. Dans l’exemple de l’analyse de sentiments, l’approche modulaire et paramétrable permet d’itérer rapidement sur différents modèles ou configurations sans perdre le fil. Résultat : on allie la vitesse d’exploration scientifique à la fiabilité d’un environnement de production, assurant la pérennité et l’impact des projets ML.\n",
    "\n",
    "(# Pensez à ajouter éventuellement un glossaire de termes techniques ou un court paragraphe “Aller plus loin” listant des liens officiels : MLflow, DVC, Hydra, Docker, etc. Cela peut aussi améliorer la lisibilité et l’enrichissement pour le lecteur.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p6-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
