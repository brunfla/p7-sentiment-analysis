{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi tester plusieurs modèles et stratégies ?\n",
    "\n",
    "Voici une vue d’ensemble des différentes stratégies de partitionnement (et pourquoi on les utilise). Cela vous aidera à clarifier :\n",
    "\n",
    "- trainTest\n",
    "- trainValTest\n",
    "- crossValidation\n",
    "\n",
    "Ces trois approches diffèrent dans leur manière de diviser les données et de gérer la validation/hyperparam tuning.\n",
    "\n",
    "### 1) Stratégie trainTest\n",
    "Principe\n",
    "\n",
    "- On fait une seule division :\n",
    "- (X_train, y_train)&(X_test, y_test)\n",
    "- Pas de jeu de validation séparé.\n",
    "- But : avoir un ensemble d’entraînement et un ensemble de test.\n",
    "\n",
    "#### Utilisation typique\n",
    "\n",
    "- Tests rapides ou prototypage.\n",
    "- Vous pouvez entraîner un modèle sur X_train, et faire un score final sur X_test.\n",
    "- Idéal si vous n’avez pas besoin de faire de tuning hyperparam (ou que vous le faites autrement).\n",
    "    - Ou, si vous faites du tuning, vous pouvez l’effectuer directement en interne à X_train (par ex. un GridSearchCV(cv=5) sur X_train), ou de manière rudimentaire (pas toujours conseillé).\n",
    "\n",
    "#### Avantages\n",
    "\n",
    "- Simplicité : un seul split.\n",
    "- Rapide : moins de code, moins de complexité.\n",
    "\n",
    "#### Inconvénients\n",
    "\n",
    "- Pas de validation explicite :\n",
    "    - Soit vous risquez de trop vous fier au score sur X_test (qui devrait être “final”, pas pour ajuster les hyperparams).\n",
    "    - Soit vous faites un mini CV sur X_train, mais c’est plus implicite.\n",
    "\n",
    "### 2) Stratégie trainValTest\n",
    "#### Principe\n",
    "\n",
    "- On fait deux splits :\n",
    "    - Train vs. Test (ex. 80% / 20%),\n",
    "    - Puis, dans le train, on refait un split en train vs. val.\n",
    "\n",
    "- Au final, vous obtenez :\n",
    "    - (X_train, y_train),(X_val, y_val),(X_test, y_test).\n",
    "\n",
    "- Utilisation typique\n",
    "    - Recherche d’hyperparamètres ou d’arrêt (early stopping) sur le jeu de val.\n",
    "    - Évaluation finale sur X_test (non utilisé pendant le tuning).\n",
    "\n",
    "#### Avantages\n",
    "\n",
    "- Séparation claire :\n",
    "    - Train : on entraîne.\n",
    "    - Val : on choisi/l’ajuste nos hyperparamètres (ou on surveille l’overfitting).\n",
    "    - Test : on obtient la performance finale, sans le “biaiser” pendant le tuning.\n",
    "\n",
    "#### Inconvénients\n",
    "\n",
    "- Moins de données pour le train :\n",
    "    - Comme on “perd” un bout pour la validation.\n",
    "- Variabilité : la performance peut dépendre de la façon dont vous coupez train/val.\n",
    "\n",
    "### 3) Stratégie crossValidation\n",
    "\n",
    "#### Principe\n",
    "\n",
    "- On fait k sous-échantillons (folds) :\n",
    "    - (train1,val1),  (train2,val2),…,(traink,valk).\n",
    "- Chaque fold valide sur un morceau différent, et on moyenne la performance (ou on cherche le meilleur hyperparam).\n",
    "- Pas de set de validation unique.\n",
    "- Souvent, on n’a pas de test final à part (ou alors, on met de côté un hold-out en plus).\n",
    "\n",
    "#### Utilisation typique\n",
    "\n",
    "- Hyperparam tuning plus robuste :\n",
    "    - On évalue l’hyperparamètre sur tous les folds => performance plus fiable.\n",
    "- Petit dataset : la cross-val permet de mieux exploiter chaque donnée (car tout le monde passe en train et en val au moins une fois).\n",
    "\n",
    "#### Avantages\n",
    "\n",
    "- Estimateur plus stable de la performance (moyenne sur k folds).\n",
    "- Moins sensible au “hasard” d’un unique split train/val.\n",
    "\n",
    "#### Inconvénients\n",
    "\n",
    "- Coût en calcul : si k=5, on entraîne 5 fois.\n",
    "- Organisation plus complexe : pour la “vraie” évaluation finale, il faut décider :\n",
    "    - Soit on reconstruit un modèle final sur toutes les données après avoir choisi l’hyperparamètre,\n",
    "    - Soit on fait un hold-out (train + crossVal interne, + test final).\n",
    "\n",
    "### Conclusion : Quel usage ?\n",
    "\n",
    "#### trainTest :\n",
    "- Rapide, simple, pour des tests exploratoires ou un proof of concept.\n",
    "- On peut faire un mini CV interne sur le train si on veut du tuning, mais c’est moins formel.\n",
    "\n",
    "#### trainValTest :\n",
    "- Structure classique dans de nombreux projets ML :\n",
    "    - Train : apprentissage,\n",
    "    - Val : tuning / early stopping,\n",
    "    - Test : score final.\n",
    "- Bonne balance entre simplicité et fiabilité.\n",
    "\n",
    "#### crossValidation :\n",
    "Pour faire un tuning plus robuste, surtout si :\n",
    "    - On a un dataset pas énorme,\n",
    "    - On veut maximiser la fiabilité de l’estimation de performance,\n",
    "    - On est prêt à payer le coup en calcul (entraînement multiple).\n",
    "    - Souvent combiné à un score moyen sur k folds.\n",
    "\n",
    "#### Ce qui crée la confusion :\n",
    "\n",
    "- Quand on fait un GridSearchCV en scikit-learn, on fait implicitement de la cross-validation à l’intérieur. On peut donc se retrouver dans un trainTest partitionneur, mais GridSearchCV(cv=5) pour le tuning. Le “test” final reste alors X_test, jamais touché pour le tuning.\n",
    "- Ou, on peut faire un partitionneur crossValidation (et l’utiliser sans GridSearchCV, ou l’utiliser en plus). Mais souvent, si on a un partitionneur crossValidation, on applique le tuning sur ces folds.\n",
    "\n",
    "\n",
    "En pratique, c’est tout à fait cohérent d'avoir un mode « validation-quick » qui ne comprend pas d’étape d’évaluation finale (ou de reporting exhaustif). L’idée est souvent de faire un test rapide :\n",
    "\n",
    "- Partition simple (train/test uniquement).\n",
    "- Entraînement (hyperparamètres par défaut ou GridSearch minimal).\n",
    "- Vérification basique (score de test).\n",
    "\n",
    "Dans le cycle de vie d’un projet de Data Science/MLOps, cela vous permet de prototyper rapidement et de décider si vous devez aller plus loin.\n",
    "\n",
    "### Scénario concret :\n",
    "Vous travaillez sur un projet d’analyse des avis clients pour un produit. Après quelques expérimentations, votre collègue propose :\n",
    "\n",
    "> \"Pourquoi ne pas tester des modèles plus avancés ? Un modèle simple fonctionne bien ici, mais peut-être qu’un modèle plus élaboré donnerait des insights plus précis, et pourrait aussi s’appliquer à d’autres cas d’usage.\"\n",
    "\n",
    "Curieux, vous décidez de tester différentes approches :\n",
    "\n",
    "1. **Commencer simple** : Vous utilisez une régression pour une validation rapide.\n",
    "2. **Approfondir** : Vous passez à un modèle avancé comme **XGBoost** pour mieux capturer les subtilités.\n",
    "3. **Maximiser les performances** : Vous testez un **BERT** afin de tirer parti de ses capacités sur des textes complexes.\n",
    "\n",
    "Lors des tests, un autre défi apparaît : votre pipeline actuel relance systématiquement toutes les étapes, y compris les plus coûteuses, même lorsque vous ne souhaitez qu’un test rapide. Cela freine vos expérimentations. Alors que vous discutez avec une collègue, elle vous lance :\n",
    "\n",
    "> \"Et si on pouvait choisir la stratégie la plus adaptée à chaque étape ? Par exemple, une validation rapide pour prototyper et une validation rigoureuse pour sélectionner le meilleur modèle.\"\n",
    "\n",
    "Cette idée vous conduit à introduire des **stratégies globales**, permettant d’ajuster le pipeline selon vos besoins.\n",
    "\n",
    "| **Étape/Stratégie**                   | **Validation rapide**             | **Optimisation équilibrée**             | **Validation rigoureuse**             |\n",
    "|-----------------------------------------|------------------------------------|------------------------------------|------------------------------------|\n",
    "| **Découpage des Données**             | 2 ensembles : Train/Test           | 3 ensembles : Train/Validation/Test | Validation croisée (k-folds)       |\n",
    "| **Cas d’usage**                       | Prototypage ou tests exploratoires | Comparaison rigoureuse            | Robustesse avec peu de données     |\n",
    "| **Optimisation**                       | Hyperparamètres par défaut        | Optimisation sur Validation        | Optimisation sur k-folds           |\n",
    "| **Validation**                         | Non applicable                    | Validation explicite              | Intégrée dans k-folds            |\n",
    "| **Promotion**                          | Usage limité                     | Standard en production             | Haute confiance pour production    |\n",
    "\n",
    "Grâce à ces ajustements, votre pipeline devient plus adaptable. Vous pouvez désormais passer rapidement d’un prototype exploratoire à une validation rigoureuse, tout en économisant du temps et des ressources.\n",
    "\n",
    "---\n",
    "\n",
    "## Mettre en Œuvre le MLOps\n",
    "\n",
    "### Outils clés :\n",
    "- **Tracking** : MLflow, Weights & Biases.\n",
    "- **Orchestration CI/CD** : GitHub Actions, Azure DevOps.\n",
    "- **Monitoring** : Azure Application Insights.\n",
    "\n",
    "### Étapes :\n",
    "1. **Initialiser et configurer** : Adoptez des configurations dynamiques avec Hydra ou DVC.\n",
    "2. **Expérimenter** : Logguez les paramètres et métriques pour chaque essai.\n",
    "3. **Valider et déployer** : Passez les modèles par des phases de staging avant la production.\n",
    "\n",
    "> **Exemple visuel : Logs d’Azure Application Insights affichant les dérives des données.**\n",
    "\n",
    "---\n",
    "\n",
    "## Boucle d’Amélioration Continue\n",
    "\n",
    "Le MLOps met en place une boucle vertueuse :\n",
    "\n",
    "1. **Collecte des données réelles** : Enrichir le dataset avec des cas non couverts.\n",
    "2. **Analyse des performances** : Suivre les métriques techniques (latence, précision) et métiers (impact sur les objectifs business).\n",
    "3. **Ajustements** : Tester de nouveaux modèles, ajuster les hyperparamètres ou changer de stratégie.\n",
    "4. **Redéploiement contrôlé** : Évaluer chaque nouvelle version avant production.\n",
    "\n",
    "> **Exemple visuel : Tableau de bord montrant les métriques d’évolution d’un modèle en production.**\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Le MLOps combine **automatisation**, **traçabilité**, et **modularité** pour rendre les pipelines adaptables. En explorant plusieurs stratégies et modèles tout en intégrant un suivi actif des performances, les entreprises peuvent réduire leurs coûts tout en améliorant leurs modèles en continu.\n",
    "\n",
    "Grâce à une approche structurée, le MLOps permet d’accélérer le temps de mise sur le marché et d’augmenter la qualité des modèles, transformant ainsi l’expérimentation en avantage compétitif durable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps : Transformer l’expérimentation en déploiement opérationnel\n",
    "\n",
    "\n",
    "| Nom de la Stratégie                           | Contexte d'Utilisation                              | Description                                                                                                        |\n",
    "|-----------------------------------------------|----------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|\n",
    "| TrainTest                                      | Prototypage rapide ou évaluation d'un modèle de base | Diviser les données en train et test. Simple et rapide, mais peu robuste. Idéal pour tester des hypothèses ou de nouvelles fonctionnalités. |\n",
    "| TrainValTest                                  | Tuning formel et test pour des datasets volumineux | Diviser les données en train, validation et test. Utiliser la validation pour l'optimisation des hyperparamètres et l'early stopping. |\n",
    "| Cross-Validation (k-fold)                     | Évaluation robuste pour des petits datasets        | Réaliser une k-fold cross-validation sur tout le dataset pour maximiser l'utilisation des données et garantir une performance fiable. |\n",
    "| TrainValTest + Cross-Validation (CV interne) | Tuning robuste et test final pour des datasets moyens à grands | Diviser les données en train, validation et test. Réaliser une cross-validation sur train pour le tuning des hyperparamètres, puis combiner train et val avant de tester sur test. |\n",
    "| TrainTest + Mini CV sur Train                  | Tuning efficace en ressources pour petits datasets ou calcul limité | Diviser simplement en train et test. Réaliser une mini cross-validation (par ex., k=3) sur train pour le tuning des hyperparamètres. Évaluer le modèle final sur test. |\n",
    "| Cross-Validation complète                     | Utilisation maximale des données pour très petits datasets | Réaliser une k-fold cross-validation sur tout le dataset sans réserver un ensemble de test. Idéal lorsque chaque exemple est critique. |\n",
    "| Cross-Validation + Hold-Out Test              | Projets critiques nécessitant un test final indépendant robuste | Réserver une petite portion (par ex., 10%) des données comme ensemble de test indépendant. Réaliser une cross-validation sur les 90% restants pour le tuning et l'évaluation avant le test final. |\n",
    "| Leave-One-Out Cross-Validation (LOOCV)       | Très petits datasets où chaque exemple est crucial | Chaque exemple est utilisé comme ensemble de test unique, et le reste est utilisé pour l'entraînement. Maximisation des données, mais coûteux en calcul. |\n",
    "| Stratified k-Fold Cross-Validation            | Datasets déséquilibrés nécessitant des folds équilibrés | Créer des folds tout en préservant la proportion des classes pour garantir une validation équilibrée dans les tâches de classification. |\n",
    "| Nested Cross-Validation                       | Évaluation fiable pour des modèles complexes avec tuning intensif | Combiner une cross-validation externe pour l'évaluation des performances et une interne pour le tuning des hyperparamètres, évitant ainsi les biais d'optimisation. |\n",
    "| Time-Series Split (Validation temporelle)     | Données séquentielles comme des séries temporelles | Diviser les données de manière séquentielle, en utilisant les données passées pour l'entraînement et les données futures pour la validation, reflétant des scénarios réels basés sur le temps. |\n",
    "| Bootstrapping                                 | Estimation d'incertitude avec petits datasets      | Générer plusieurs échantillons avec remise pour une évaluation flexible et une estimation d'intervalles de confiance. |\n",
    "\n",
    "L’un des avantages majeurs d’un pipeline MLOps est la paramétrabilité :\n",
    "On peut passer d’un mode rapide (trainTest léger) à un mode exhaustif (crossVal, tuning d’hyperparamètres) sans tout réécrire,\n",
    "- Les modifications sont généralement centralisées via des fichiers de configuration (YAML, JSON…) ou via une interface dédiée.\n",
    "\n",
    "##  3. Rôle du MLOps : orchestrer le cycle de vie des builds\n",
    "\n",
    "### 3.1 Concrètement, qu’apporte un pipeline MLOps ?\n",
    "\n",
    "- Préparation des données\n",
    "    - Automatisation de la collecte et du nettoyage,\n",
    "    - Possibilité de choisir dynamiquement la stratégie de split (trainTest, trainValTest, crossVal).\n",
    "\n",
    "- Entraînement et validation\n",
    "    - Lancement de plusieurs runs avec différents modèles (Logistic Regression, XGBoost, BERT…),\n",
    "    - Calcul et enregistrement des métriques (accuracy, F1, AUC…),\n",
    "    - Possibilité de configurer rapidement de nouveaux paramètres (learning rate, nombre d’époques, etc.).\n",
    "\n",
    "- Comparaison et sélection\n",
    "    - Centralisation des résultats dans un outil de suivi (MLflow, Weights & Biases, etc.),\n",
    "    - Visualisation et comparaison des scores pour déterminer le « meilleur » modèle ou la meilleure configuration.\n",
    "\n",
    "- Déploiement en production\n",
    "    - Création d’un service ou d’une API qui expose le modèle,\n",
    "    - Versionnement pour garder la trace du modèle déployé (et rollback si nécessaire).\n",
    "\n",
    "- Monitoring et alertes\n",
    "    - Surveiller la performance en situation réelle (taux d’erreur, latence, dérive de distribution),\n",
    "    - Déclencher un nouveau cycle d’entraînement ou alerter l’équipe lorsque le modèle se dégrade.\n",
    "\n",
    "##  4. Étapes MLOps essentielles : du monitoring à la production\n",
    "\n",
    "### 4.1. Importance du monitoring\n",
    "\n",
    "Même un modèle très performant hors ligne peut se dégrader avec le temps. Les raisons sont multiples :\n",
    "\n",
    "- Changement de la distribution : les données en production ne ressemblent plus à celles du dataset initial (évolution des tendances, nouveaux types d’utilisateurs…),\n",
    "- Nouveaux cas d’usage : l’entreprise décide d’utiliser le modèle pour un objectif légèrement différent,\n",
    "- Vieillissement du modèle : les patterns apprises s’avèrent moins pertinentes face à des données nouvelles.\n",
    "\n",
    "### 4.2. Mise en production et suivi\n",
    "\n",
    "- Déploiement continu (CD) : un modèle validé par les data scientists peut être déployé directement via un pipeline automatisé,\n",
    "- A/B testing ou canary release : on sert le nouveau modèle à une fraction d’utilisateurs pour s’assurer que l’impact est positif,\n",
    "- Monitoring : collecte en continu des prédictions et des données réelles, comparaison des distributions, alerte en cas de dérive ou de baisse de performance.\n",
    "\n",
    "### 4.3. Retour au pipeline MLOps\n",
    "\n",
    "En cas de détection de dérive ou de baisse de performance, on revient aux étapes précédentes :\n",
    "\n",
    "- Ajout de données plus récentes,\n",
    "- Ajustement des hyperparamètres ou test d’un nouveau modèle,\n",
    "- Re-lancement de l’entraînement, validation, puis re-déploiement si les résultats s’améliorent.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "En synthèse :\n",
    "\n",
    "- Choix du modèle : commencer par un baseline pour disposer d’une référence, puis envisager des modèles plus sophistiqués si les performances stagnent.\n",
    "- Stratégie de validation : adapter le découpage (trainTest, trainValTest, crossValidation) selon la phase du projet (prototypage, tuning, finalisation).\n",
    "- Besoin de flexibilité : pouvoir switcher facilement entre plusieurs approches (nouveaux algorithmes, nouvelles métriques, nouveaux preprocessings) grâce à des pipelines paramétrables.\n",
    "- MLOps : mettre en place l’automatisation, la traçabilité, l’orchestration et le monitoring pour gérer le cycle de vie complet du modèle – de la première expérimentation jusqu’au suivi en production.\n",
    "\n",
    "En adoptant cette démarche, les équipes ML peuvent itérer rapidement et gérer la complexité des multiples expérimentations, tout en garantissant une qualité et une réplicabilité qui deviendront essentielles au moment de déployer le modèle dans un contexte métier. Le MLOps s’affirme ainsi comme le levier indispensable pour faire évoluer efficacement les projets de Machine Learning, en créant un environnement où chaque nouvelle idée peut être testée, évaluée, et déployée dans un cadre à la fois sûr et agile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps : de l’expérimentation au déploiement opérationnel\n",
    "\n",
    "Les projets de Machine Learning (ML) sont souvent comparés à des expériences scientifiques : un processus itératif où chaque expérimentation — qu’il s’agisse de tester un modèle, d’ajuster les données ou de modifier une stratégie de validation — permet d’apprendre et d’améliorer progressivement les approches. Cependant, ce caractère expérimental, bien qu’essentiel pour optimiser les performances, introduit des défis complexes lorsqu’il s’agit de transformer ces apprentissages en solutions robustes et fiables en production.\n",
    "\n",
    "C’est précisément ici que le MLOps (Machine Learning Operations) intervient. Inspiré du DevOps, le MLOps apporte des outils et des pratiques pour gérer les spécificités des projets ML, comme la dépendance aux données, la reproductibilité des expérimentations et le suivi continu des performances des modèles en production. En structurant et en automatisant chaque étape du cycle de vie ML, il permet aux équipes de surmonter les défis de l’expérimentation, tout en accélérant le passage à l’échelle.\n",
    "\n",
    "Par exemple, la sélection des modèles illustre bien l’importance d’un cadre structuré. Identifier le modèle optimal implique de tester différentes configurations, comparer leurs performances et itérer rapidement. Or, sans un suivi rigoureux et des pipelines bien définis, il devient difficile de gérer la complexité croissante des expérimentations. Ce besoin de reproductibilité, de collaboration et d’automatisation se retrouve dans chaque étape clé d’un projet ML, depuis la préparation des données jusqu’au déploiement des modèles.\n",
    "\n",
    "Au-delà de la sélection des modèles, des défis comme la gestion des dérives en production, le versioning des données et des modèles, ou encore la scalabilité des ressources rendent indispensable une approche comme le MLOps. Ces défis, au cœur des projets ML modernes, nécessitent une orchestration fine pour transformer des idées en solutions opérationnelles, fiables et scalables.\n",
    "\n",
    "## 1. Environnements cohérents : une base essentielle pour la reproductibilité\n",
    "\n",
    "Dans le Machine Learning, les résultats expérimentaux dépendent fortement de l’environnement dans lequel les modèles sont entraînés et validés. Une différence dans une version de bibliothèque ou une configuration matérielle peut entraîner des résultats divergents, compromettant la reproductibilité et la fiabilité des modèles. C’est pourquoi garantir des environnements cohérents est une priorité. Inspiré du DevOps, le MLOps s’appuie sur des outils comme Docker et Kubernetes pour encapsuler les dépendances et standardiser les environnements, que ce soit sur un poste local, un cluster GPU ou en production. Cela permet aux équipes de collaborer efficacement, en ayant l’assurance que chaque expérimentation est reproductible, et pose les fondations nécessaires à l’automatisation des workflows.\n",
    "\n",
    "## 2. Automatisation des workflows : accélérer les expérimentations\n",
    "\n",
    "Dans un contexte expérimental, les tâches répétitives comme la préparation des données, l’entraînement des modèles et la validation des résultats peuvent rapidement devenir un goulet d’étranglement. Le MLOps automatise ces étapes grâce à des pipelines définis pour exécuter chaque phase de manière standardisée et fiable. Par exemple, des outils comme Kubeflow, MLflow, ou encore GitHub Actions orchestrent les workflows, garantissant que les expérimentations sont exécutées efficacement et avec un minimum d’erreurs humaines. Cette automatisation accélère non seulement les cycles itératifs mais assure aussi que les configurations et résultats sont bien documentés, ce qui est indispensable pour la traçabilité.\n",
    "\n",
    "## 3. Traçabilité : comprendre et reproduire chaque étape\n",
    "\n",
    "La traçabilité est un pilier du MLOps. Dans un projet ML, il est crucial de savoir quelles données ont été utilisées, quels hyperparamètres ont été testés et quelles versions de modèles ont produit quels résultats. Sans cette documentation systématique, il devient impossible de diagnostiquer des problèmes ou de reproduire des résultats. En intégrant le versioning des modèles, des données et des configurations, des outils comme MLflow ou DVC garantissent une transparence totale. Cela ne facilite pas seulement le suivi des expérimentations, mais constitue aussi une base solide pour des audits ou des analyses approfondies, en particulier dans des contextes où la régulation et la conformité sont importantes.\n",
    "\n",
    "## 4. Déploiement fluide : passer de l’expérimentation à la production\n",
    "\n",
    "Une fois un modèle validé, le passage en production peut être un défi si les environnements d’entraînement diffèrent de ceux de déploiement. Le MLOps, grâce à des workflows automatisés et reproductibles, facilite cette transition. Des solutions comme CI/CD pour modèles ML permettent d’intégrer et de déployer les modèles validés dans des environnements de production de manière fluide et sécurisée. De plus, les pipelines MLOps s’assurent que chaque modèle déployé peut être monitoré, mis à jour ou remplacé sans interrompre les systèmes en production, rendant le processus à la fois robuste et adaptable.\n",
    "\n",
    "## 5. Monitoring : surveiller et maintenir les performances des modèles\n",
    "\n",
    "Le travail ne s’arrête pas au déploiement. Une fois en production, un modèle doit être supervisé pour détecter les dérives des données ou les baisses de performance, qui peuvent survenir à mesure que les conditions évoluent. Le monitoring continu, une pratique issue du DevOps, est adapté au ML pour suivre non seulement les performances du système, mais aussi des métriques spécifiques aux modèles, comme la précision ou la latence. Lorsqu’une dégradation est détectée, des alertes peuvent déclencher des workflows automatiques pour réentraîner le modèle, ajuster ses paramètres ou déployer une nouvelle version. Cela garantit que les modèles restent fiables, même dans des environnements dynamiques et changeants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Intégrer les pratiques DevOps au Machine Learning\n",
    "\n",
    "Le MLOps s’appuie sur les principes fondamentaux du DevOps, comme l’intégration continue (CI) et le déploiement continu (CD), mais adapte ces concepts aux besoins spécifiques des projets de Machine Learning. En DevOps, un pipeline CI/CD garantit que chaque modification de code est testée et déployée de manière fiable. Cependant, dans le contexte du ML, le pipeline ne se limite pas à la gestion du code : il englobe aussi la manipulation des données, l’entraînement des modèles et la validation des résultats, rendant le processus plus complexe et nécessitant des outils dédiés.\n",
    "Différences entre un pipeline DevOps et un pipeline MLOps\n",
    "\n",
    "Un pipeline DevOps se concentre sur le développement logiciel traditionnel, où le produit final est un code source déployé dans un environnement stable. En revanche, un pipeline MLOps doit gérer des artefacts supplémentaires et dynamiques, comme :\n",
    "- **Les modèles ML**, qui évoluent en fonction des données et des hyperparamètres.\n",
    "- **Les jeux de données**, dont les modifications impactent directement les résultats.\n",
    "- **Les métriques de performance**, qui ne se limitent pas à des tests unitaires mais nécessitent des comparaisons et analyses approfondies.\n",
    "\n",
    "Ces différences impliquent que, bien qu’un pipeline MLOps puisse intégrer certains éléments d’un pipeline DevOps, il doit être élargi et spécialisé pour gérer les aspects uniques du ML.\n",
    "\n",
    "## 2. Les étapes spécifiques d’un pipeline MLOps\n",
    "\n",
    "Un pipeline MLOps reprend la structure logique des pipelines DevOps tout en intégrant les étapes propres au cycle de vie d’un modèle ML. Ces étapes permettent de standardiser et d’automatiser l’ensemble du processus, garantissant cohérence, reproductibilité et scalabilité :\n",
    "\n",
    "    Préparation des données\n",
    "        Ingestion, nettoyage et transformation des données brutes pour les rendre exploitables.\n",
    "        Automatisation des pipelines de données pour garantir une cohérence entre les expérimentations.\n",
    "\n",
    "    Entraînement des modèles\n",
    "        Orchestration des configurations et des hyperparamètres pour optimiser les performances.\n",
    "        Utilisation d’outils comme Kubeflow ou MLflow pour standardiser les entraînements.\n",
    "\n",
    "    Validation des performances\n",
    "        Comparaison systématique des modèles à l’aide de métriques définies (précision, rappel, etc.).\n",
    "        Sélection de la meilleure configuration pour répondre aux exigences métier.\n",
    "\n",
    "    Déploiement\n",
    "        Mise en production des modèles validés via des workflows traçables.\n",
    "        Suivi des performances du modèle en environnement réel pour détecter tout dérèglement.\n",
    "\n",
    "Ces étapes, orchestrées dans un pipeline MLOps, permettent de transformer l’expérimentation en un processus fiable et reproductible, tout en offrant la flexibilité d’intégrer de nouvelles données ou configurations.\n",
    "\n",
    "\n",
    "Prenons un **exemple concret** : Vous travaillez sur un projet d'analyse de sentimets. Après avoir testé un simple modèle de classification basé sur un **régression logistique** pour valider l’idée, vous passez a des approches plus avancées, comme les **word embeddings** combinés à des réseaux de neurones profonds (CNN ou RNN) pour capturer les relations contextuelles et ainsi améliorer la performance. Enfin, pour maximiser la performance, des modèles d'État de l'art comme BERT peuvent être mis en œuvre.\n",
    "\n",
    "\n",
    "## 1. Un pipeline dynamique grâce au MLOps\n",
    "\n",
    "Le MLOps permet aussi de **passer d’une expérience à une autre de manière fluide**, tout en facilitant la comparaison des résultats. Ce cadre structuré et dynamique est essentiel pour gérer la complexité croissante des projets ML, réduire les coûts d’expérimentation, et garantir un déploiement robuste des modèles.\n",
    "\n",
    "Parfois, pour tester une hypothèse, une solution simple peut suffire pour atteindre les objectifs, tandis que dans d’autres cas, une modélisation plus sophistiquée s’impose. \n",
    "Dans un tel contexte, la pipeline doit pouvior être suffisament flexible pour s'adapter à divers scénario. Par exemple, grâce à des configurations centralisées, il est possible d’ajuster facilement les modèles ou les stratégies à tester sans altérer la structure globale du pipeline. \n",
    "\n",
    "---> Capture d'ecran de hydra : *stratégies adaptées à chaque phase*\n",
    "\n",
    "\n",
    "\n",
    "relancer systématiquement **toutes les étapes complexes** – de la préparation des données à la validation rigoureuse, à chaque nouvelle hypothèse serait inefficace, coûteux, et chronophage. Il est donc essentiel de :\n",
    "- Utiliser des **stratégies adaptées à chaque phase** (par exemple, valider une hypothèse avec un simple découpage *train/test*, puis passer à une validation croisée rigoureuse une fois le modèle stabilisé).  \n",
    "- Comparer les expériences sans nécessairement reproduire intégralement les précédentes (exploiter des outils comme MLflow pour centraliser les résultats). \n",
    "\n",
    "\n",
    "## OPtimisatin des ressources\n",
    "Cependant, à chaque nouvelle étape, les ressources nécessaires augmentent :  \n",
    "- En phase initiale, un **régression logistique** peut être entraîné en quelques minutes, ce qui permet d’itérer rapidement.  \n",
    "- Avec un CNN, l’entraînement peut nécessiter plusieurs heures, voire plusieurs jours si les données sont volumineuses.\n",
    "- Si vous fine-tunez un modèle comme BERT, cette phase peut être réalisée en quelques heures ou quelques jours avec des GPU adaptés, bien que les besoins en mémoire et en calcul restent élevés.\n",
    "\n",
    "Le MLOps joue un rôle déterminant pour rendre ce processus itératif plus efficace :\n",
    "- **Automatisation des expérimentations** : Les outils comme MLflow permettent d’enregistrer chaque configuration et résultat, ce qui simplifie la comparaison entre les modèles.\n",
    "- **Traçabilité** : Chaque expérimentation est documentée (modèles, paramètres, jeux de données utilisés), ce qui garantit la reproductibilité et facilite la collaboration entre équipes.\n",
    "- **Flexibilité** : Grâce à des configurations centralisées, il est possible d’ajuster facilement les modèles ou les stratégies à tester sans altérer la structure globale du pipeline.\n",
    "- **Gestion des ressources** : Des pipelines bien conçus, avec des orchestrateurs comme Kubeflow, permettent de tester rapidement des modèles simples et de réserver les ressources coûteuses (comme les GPU) aux étapes critiques.\n",
    "\n",
    "De plus, Le rôle du MLOps inclut l'**optimisation des processus** d'expérimentation en fournissant des informations pratiques sur le coût et le temps nécessaires pour valider une hypothèse. Cela permet au ML Engineer de prioriser intelligemment les hypothèses à tester en fonction des ressources disponibles et des contraintes du projet. Les outils MLOps, comme MLflow, Kubeflow, ou encore des orchestrateurs comme Airflow, peuvent traquer les expérimentations passées et fournir des métriques sur les temps d'entraînement et les ressources utilisées pour des modèles similaires.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.2. Pourquoi varier les modèles et les stratégies ?\n",
    "\n",
    "Les besoins d’un projet ML changent selon sa phase :  \n",
    "- **Phase initiale** : Tests rapides avec un **TrainTest** pour valider une idée.  \n",
    "- **Phase d’optimisation** : Utilisation de **TrainValTest** ou de **Cross-Validation** pour affiner les hyperparamètres.  \n",
    "- **Phase critique** : Approches rigoureuses comme la **Nested Cross-Validation** pour garantir une évaluation fiable.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Evaluer les modèles\n",
    "\n",
    "Après avoir entrainé les modèles candidats, la deuxième grande question est : comment valider leur performance et comparer les résultats de manière fiable ?\n",
    "La validation est essentielle pour mesurer la fiabilité et la généralisation des modèles sur de nouvelles données. Selon le contexte, différentes stratégies peuvent être utilisées.\n",
    "\n",
    "Prenons un **exemple concret** : Vous travaillez sur un projet d'analyse de sentimets. Après avoir testé un simple modèle de classification basé sur un **régression logistique** pour valider l’idée, vous passez a des approches plus avancées, comme les **word embeddings** combinés à des réseaux de neurones profonds (CNN ou RNN) pour capturer les relations contextuelles et ainsi améliorer la performance. Enfin, pour maximiser la performance, des modèles d'État de l'art comme BERT peuvent être mis en œuvre.\n",
    "\n",
    "En réalité, il existe plusieurs stratégies d'évaluation, qui sera adapté pour \n",
    "\n",
    "### 2.1. Aperçu des stratégies de validation courantes\n",
    "\n",
    "| **Nom de la stratégie**                   | **Utilisation typique**                          | **Principe**                                                                                       |\n",
    "|--------------------------------------------|--------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n",
    "| **TrainTest**                              | Prototypage rapide, modèle de base               | Diviser les données en deux ensembles : entraînement (train) et test. Simple et rapide. |\n",
    "| **TrainValTest**                           | Tuning formel pour des datasets volumineux       | Ajouter un ensemble validation (val) pour l'optimisation des hyperparamètres. |\n",
    "| **Cross-Validation (k-fold)**              | Évaluation robuste sur des datasets modérés      | Répartir les données en k sous-ensembles (folds) pour évaluer sur chaque fold. |\n",
    "| **Validation temporelle (Time-Series)**    | Données séquentielles (séries temporelles)       | Respecter l’ordre chronologique pour éviter les fuites de données. |\n",
    "| **Nested Cross-Validation**                | Modèles complexes avec tuning intensif           | Combiner une validation interne (hyperparamètres) et externe (évaluation finale). |\n",
    "\n",
    "\n",
    "\n",
    "Donner un exemple contrat, par exemple citez comme exemple, que pour valider une hypothèse, on a pas forcement besoin de relancer toutes les étapes complexes. Il est intéressant je pense de rappeler que les ressources lié à l'entrainement sont gourmand et peuvent durée des mois.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Quels outils et processus le MLOps met en place pour relever ces défis ?\n",
    "\n",
    "Le MLOps offre un ensemble de pratiques et d’outils pour répondre aux problèmes cités dans les sections précédentes. Avant d’entrer dans le détail, il est utile de comprendre les similitudes et différences entre DevOps et MLOps.\n",
    "\n",
    "### 3.1. MLOps vs. DevOps : des similarités et des spécificités\n",
    "\n",
    "| **Aspect**               | **DevOps**                               | **MLOps**                               |\n",
    "|--------------------------|------------------------------------------|-----------------------------------------|\n",
    "| **Automatisation**       | Pipelines CI/CD pour le déploiement du code. | Pipelines pour données, modèles et entraînements. |\n",
    "| **Collaboration**        | Développeurs et Ops travaillent ensemble. | Data scientists, ingénieurs ML, et Ops collaborent. |\n",
    "| **Complexité**           | Focus sur le code et l’infrastructure.   | Intègre la gestion des données, des modèles et du code. |\n",
    "| **Monitoring**           | Latence, disponibilité des systèmes.     | Performance des modèles (précision, dérive des données). |\n",
    "\n",
    "En résumé, le **MLOps** étend les principes du DevOps à la gestion des données et des modèles, avec des cycles d’entraînement spécifiques.\n",
    "\n",
    "### 3.2. Les piliers du MLOps : outils et processus essentiels\n",
    "\n",
    "#### Automatisation\n",
    "- **Collecte et préparation des données** :\n",
    "  - **Outils** : ETL (Extract, Transform, Load) automatisés.\n",
    "  - **Avantage** : Gain de temps et réduction des erreurs manuelles.  \n",
    "- **Entraînement des modèles** :\n",
    "  - **Outils** : Kubeflow, Airflow.\n",
    "  - **Avantage** : Simplifie les tests multiples et le tuning des hyperparamètres.\n",
    "\n",
    "#### Orchestration\n",
    "- **Gestion centralisée des configurations** :\n",
    "  - **Outils** : Hydra, YAML.\n",
    "  - **Avantage** : Modifier facilement les stratégies (TrainTest, CrossVal).  \n",
    "- **Workflow structuré** :\n",
    "  - **Outils** : Airflow, Dagster.\n",
    "  - **Avantage** : Organiser toutes les étapes du pipeline (prétraitement, entraînement, validation, déploiement).\n",
    "\n",
    "#### Suivi et traçabilité\n",
    "- **Tracking des expérimentations** :\n",
    "  - **Outils** : MLflow, DVC.\n",
    "  - **Avantage** : Permet une comparaison rigoureuse des modèles.  \n",
    "- **Versionnement des données et des modèles** :\n",
    "  - **Outils** : Git LFS, DVC.\n",
    "  - **Avantage** : Garantit la reproductibilité des expériences.\n",
    "\n",
    "#### Déploiement et monitoring\n",
    "- **Création de microservices pour les modèles** :\n",
    "  - **Outils** : Docker, FastAPI.\n",
    "  - **Avantage** : Intégration facile dans des environnements réels.  \n",
    "- **Monitoring actif** :\n",
    "  - **Outils** : Prometheus, Evidently AI.\n",
    "  - **Avantage** : Surveille les performances en production pour détecter les dérives.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Assurer un suivi continu et une adaptabilité en production\n",
    "\n",
    "### 4.1. Pourquoi le monitoring est essentiel\n",
    "\n",
    "Un modèle performant hors ligne peut se dégrader en production pour diverses raisons :\n",
    "- **Changement des données** : Les données évoluent, impactant la pertinence des prédictions.\n",
    "- **Dérive des cas d’usage** : L’application du modèle change avec le temps.\n",
    "- **Vieillissement du modèle** : Le modèle devient obsolète face à des patterns nouveaux.\n",
    "\n",
    "### 4.2. Adapter le pipeline en continu grâce au MLOps\n",
    "\n",
    "Lorsqu’un problème est détecté :\n",
    "1. **Récupérer de nouvelles données** pour réentraîner le modèle.  \n",
    "2. **Ajuster les hyperparamètres** ou tester une nouvelle stratégie de validation.  \n",
    "3. **Re-déployer le modèle** après validation, tout en gardant la trace des modifications.  \n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Le **MLOps** permet d’orchestrer efficacement l’ensemble du cycle de vie d’un projet ML :\n",
    "1. Automatiser les tâches répétitives pour accélérer les expérimentations.  \n",
    "2. Centraliser et tracer les configurations pour faciliter la reproductibilité.  \n",
    "3. Assurer une adaptabilité continue en production grâce au monitoring actif.\n",
    "\n",
    "En intégrant le MLOps dans leurs workflows, les équipes ML peuvent innover plus rapidement, tout en garantissant des modèles fiables, évolutifs, et robustes en production.  \n",
    "\n",
    "Le **MLOps** n’est pas simplement un outil ; c’est un **levier stratégique** pour transformer des expérimentations en succès opérationnels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps : de l’expérimentation au déploiement opérationnel\n",
    "\n",
    "Les projets de Machine Learning (ML) sont souvent comparés à des expériences scientifiques : un processus itératif où chaque expérimentation, qu’il s’agisse de tester un modèle, d’ajuster les données ou de modifier une stratégie de validation, permet d’apprendre et d’améliorer progressivement les approches. Cependant, ce caractère expérimental, bien qu’essentiel pour optimiser les performances, introduit des défis complexes lorsqu’il s’agit de transformer ces apprentissages en solutions robustes et fiables en production.\n",
    "\n",
    "La sélection des modèles illustre bien l’importance d’un cadre structuré. Prenons un **exemple concret** : Vous travaillez sur un projet d'analyse de sentimets. Après avoir testé un simple modèle de classification basé sur un **régression logistique** pour valider l’idée, vous passez a des approches plus avancées, comme les **word embeddings** combinés à des réseaux de neurones profonds (CNN ou RNN) pour capturer les relations contextuelles et ainsi améliorer la performance. Enfin, pour maximiser la performance, des modèles d'État de l'art comme BERT peuvent être mis en œuvre.\n",
    "\n",
    "Dans ce contexte, identifier le modèle optimal implique de tester différentes configurations, comparer leurs performances et itérer rapidement. Or, sans un suivi rigoureux et des pipelines bien définis, il devient difficile de gérer la complexité croissante des expérimentations. \n",
    "\n",
    "Ce besoin de reproductibilité, de collaboration et d’automatisation se retrouve dans chaque étape clé d’un projet ML, depuis la préparation des données jusqu’au déploiement des modèles.\n",
    "\n",
    "C’est précisément ici que le MLOps (Machine Learning Operations) intervient. Inspiré du DevOps, le MLOps apporte des outils et des pratiques pour gérer les spécificités des projets ML, comme la dépendance aux données, la reproductibilité des expérimentations et le suivi continu des performances des modèles en production. En structurant et en automatisant chaque étape du cycle de vie ML, il permet aux équipes de surmonter les défis de l’expérimentation, tout en accélérant le passage à l’échelle.\n",
    "\n",
    "Exemple de capture d'écran de github:\n",
    "\n",
    "\n",
    "\n",
    "## 1. Environnements cohérents : une base essentielle pour la reproductibilité\n",
    "\n",
    "\n",
    "Au-delà de la sélection des modèles, des défis comme la gestion des dérives en production, le versioning des données et des modèles, ou encore la scalabilité des ressources rendent indispensable une approche comme le MLOps. Ces défis, au cœur des projets ML modernes, nécessitent une orchestration fine pour transformer des idées en solutions opérationnelles, fiables et scalables.\n",
    "\n",
    "Dans le Machine Learning, les résultats expérimentaux dépendent fortement de l’environnement dans lequel les modèles sont entraînés et validés. Une différence dans une version de bibliothèque ou une configuration matérielle peut entraîner des résultats divergents, compromettant la reproductibilité et la fiabilité des modèles. C’est pourquoi garantir des environnements cohérents est une priorité. Inspiré du DevOps, le MLOps s’appuie sur des outils comme Docker et Kubernetes pour encapsuler les dépendances et standardiser les environnements, que ce soit sur un poste local, un cluster GPU ou en production. Cela permet aux équipes de collaborer efficacement, en ayant l’assurance que chaque expérimentation est reproductible, et pose les fondations nécessaires à l’automatisation des workflows.\n",
    "\n",
    "## 2. Automatisation des workflows : accélérer les expérimentations\n",
    "\n",
    "Dans un contexte expérimental, les tâches répétitives comme la préparation des données, l’entraînement des modèles et la validation des résultats peuvent rapidement devenir un goulet d’étranglement. Le MLOps automatise ces étapes grâce à des pipelines définis pour exécuter chaque phase de manière standardisée et fiable. Par exemple, des outils comme Kubeflow, MLflow, ou encore GitHub Actions orchestrent les workflows, garantissant que les expérimentations sont exécutées efficacement et avec un minimum d’erreurs humaines. Cette automatisation accélère non seulement les cycles itératifs mais assure aussi que les configurations et résultats sont bien documentés, ce qui est indispensable pour la traçabilité.\n",
    "\n",
    "## 3. Traçabilité : comprendre et reproduire chaque étape\n",
    "\n",
    "La traçabilité est un pilier du MLOps. Dans un projet ML, il est crucial de savoir quelles données ont été utilisées, quels hyperparamètres ont été testés et quelles versions de modèles ont produit quels résultats. Sans cette documentation systématique, il devient impossible de diagnostiquer des problèmes ou de reproduire des résultats. En intégrant le versioning des modèles, des données et des configurations, des outils comme MLflow ou DVC garantissent une transparence totale. Cela ne facilite pas seulement le suivi des expérimentations, mais constitue aussi une base solide pour des audits ou des analyses approfondies, en particulier dans des contextes où la régulation et la conformité sont importantes.\n",
    "\n",
    "## 4. Déploiement fluide : passer de l’expérimentation à la production\n",
    "\n",
    "Une fois un modèle validé, le passage en production peut être un défi si les environnements d’entraînement diffèrent de ceux de déploiement. Le MLOps, grâce à des workflows automatisés et reproductibles, facilite cette transition. Des solutions comme CI/CD pour modèles ML permettent d’intégrer et de déployer les modèles validés dans des environnements de production de manière fluide et sécurisée. De plus, les pipelines MLOps s’assurent que chaque modèle déployé peut être monitoré, mis à jour ou remplacé sans interrompre les systèmes en production, rendant le processus à la fois robuste et adaptable.\n",
    "\n",
    "## 5. Monitoring : surveiller et maintenir les performances des modèles\n",
    "\n",
    "Le travail ne s’arrête pas au déploiement. Une fois en production, un modèle doit être supervisé pour détecter les dérives des données ou les baisses de performance, qui peuvent survenir à mesure que les conditions évoluent. Le monitoring continu, une pratique issue du DevOps, est adapté au ML pour suivre non seulement les performances du système, mais aussi des métriques spécifiques aux modèles, comme la précision ou la latence. Lorsqu’une dégradation est détectée, des alertes peuvent déclencher des workflows automatiques pour réentraîner le modèle, ajuster ses paramètres ou déployer une nouvelle version. Cela garantit que les modèles restent fiables, même dans des environnements dynamiques et changeants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLOps : Quand le Machine Learning rencontre l’esprit du DevOps\n",
    "\n",
    "Le Machine Learning, on aime souvent le comparer à des expériences scientifiques : on teste, on ajuste, on explore. Une sorte de laboratoire où chaque idée – que ce soit un modèle, une façon de traiter les données ou une stratégie de validation – est une nouvelle hypothèse qu’on met à l’épreuve. Mais, soyons honnêtes, cette comparaison s’arrête vite quand il s’agit de passer en production.\n",
    "\n",
    "Déployer un modèle, ce n’est pas juste une question de « ça marche sur mon notebook ». Là, on rentre dans une logique plus sérieuse : on parle de scalabilité, de maintenance, et de résultats fiables. Et pour ça, le parallèle avec les pratiques DevOps devient beaucoup plus pertinent. Après tout, ce qu’a fait DevOps pour le développement logiciel – en automatisant, en structurant, et en favorisant la collaboration – c’est exactement ce dont on a besoin pour le Machine Learning. Bienvenue dans l’univers du MLOps.\n",
    "Pourquoi le DevOps est une bonne analogie pour le MLOps\n",
    "\n",
    "Vous voyez, dans un projet DevOps classique, le but est clair : produire du code fiable, rapidement et efficacement, tout en garantissant que tout fonctionne comme prévu une fois déployé. On utilise des pipelines, des tests automatisés, et des outils pour surveiller les applications en production. En Machine Learning, l’idée est la même… mais avec quelques twists.\n",
    "1. Ce n’est pas juste une histoire de code\n",
    "\n",
    "En DevOps, on s’occupe principalement du code. En MLOps, il faut aussi gérer les données et les modèles. Et là, les problèmes explosent :\n",
    "\n",
    "    Les données évoluent : Un modèle qui marchait hier peut devenir obsolète demain si les données changent (on appelle ça le drift).\n",
    "    La gestion des versions : Imaginez qu’on vous demande pourquoi le modèle de la semaine dernière était meilleur. Si vous ne versionnez pas les données et les hyperparamètres, bon courage pour répondre.\n",
    "\n",
    "2. Les pipelines ne sont plus optionnels\n",
    "\n",
    "Un pipeline DevOps classique, c’est déjà impressionnant : vous codez, vous poussez sur Git, et boum, tout se construit et se déploie automatiquement. En MLOps, on va un cran plus loin :\n",
    "\n",
    "    Préparer les données (nettoyage, enrichissement).\n",
    "    Entraîner les modèles (et tester plusieurs configurations).\n",
    "    Valider les résultats (cross-validation, ensembles de test).\n",
    "    Et ensuite ? Surveiller en continu. Parce qu’un modèle, contrairement à du code, peut se dégrader avec le temps.\n",
    "\n",
    "3. Le monitoring devient critique\n",
    "\n",
    "En DevOps, vous surveillez des métriques comme le taux d’erreurs ou la latence. En MLOps, on surveille aussi… mais différemment :\n",
    "\n",
    "    Votre modèle fait-il toujours des prédictions fiables ?\n",
    "    Les données que vous recevez en production ressemblent-elles à celles que vous aviez pour l’entraînement ?\n",
    "    Et surtout, les résultats sont-ils encore utiles pour le business ?\n",
    "\n",
    "Exemple concret : De l’analyse de sentiments à MLOps\n",
    "\n",
    "Prenons un exemple : vous bossez sur un projet d’analyse de sentiments pour des avis clients.\n",
    "\n",
    "    Vous commencez simple, avec une régression logistique. Pourquoi pas, ça marche souvent bien pour valider une idée.\n",
    "    Ensuite, vous montez en puissance avec des word embeddings et des réseaux neuronaux pour capter les nuances contextuelles.\n",
    "    Enfin, vous passez à du lourd : un modèle comme BERT, qui peut gérer des subtilités linguistiques complexes.\n",
    "\n",
    "C’est super ! Mais maintenant, il faut industrialiser tout ça :\n",
    "\n",
    "    Pipeline automatisé : Chaque nouvelle donnée doit passer par les mêmes étapes que celles que vous avez utilisées pour entraîner votre modèle. Pas de place pour les raccourcis.\n",
    "    Versionning rigoureux : Quel modèle avez-vous utilisé ? Quelles données ? Quels hyperparamètres ? Tout doit être traçable.\n",
    "    Monitoring en production : Si les avis clients commencent à utiliser un langage complètement nouveau (emoji, argot, etc.), votre modèle doit le détecter… ou il deviendra inutile.\n",
    "\n",
    "Alors, pourquoi tout ce bruit autour du MLOps ?\n",
    "\n",
    "C’est simple : on ne peut plus se permettre d’avoir des modèles qui « marchent juste dans le notebook ». Les projets Machine Learning sont devenus stratégiques, et le MLOps est là pour s’assurer qu’ils tiennent leurs promesses. En s’appuyant sur les idées du DevOps – comme les pipelines, l’automatisation et le monitoring – et en y ajoutant des outils spécifiques pour gérer les données et les modèles, le MLOps fait le lien entre expérimentation et production.\n",
    "\n",
    "Conclusion : Une évolution naturelle inspirée du DevOps\n",
    "\n",
    "Si le DevOps a permis aux équipes de développement de livrer du code fiable plus rapidement, le MLOps applique ces mêmes principes au Machine Learning. Mais ici, la complexité est décuplée : les données changent, les modèles vieillissent, et l’impact sur le business peut être énorme. En structurant chaque étape – de la préparation des données au monitoring en production – le MLOps transforme les idées de Machine Learning en solutions concrètes et durables. Et ça, c’est un vrai game changer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps : Quand le Machine Learning rencontre l’esprit du DevOps\n",
    "\n",
    "Les projets de Machine Learning (ML) sont souvent comparés à des expériences scientifiques : on explore, on teste, on ajuste. Une sorte de laboratoire où chaque idée est mise à l’épreuve. Mais cette approche, bien qu’efficace pour apprendre, montre vite ses limites quand il s’agit de passer à la production.\n",
    "\n",
    "Avec des modèles plus complexes, plus de données, et des besoins de monitoring, il ne suffit plus que « ça marche dans le notebook ». C’est ici que le MLOps, inspiré du DevOps, entre en jeu : il structure, automatise et industrialise chaque étape d’un projet ML. Et pour le démontrer, prenons un exemple concret : un projet d’analyse de sentiments.\n",
    "\n",
    "## Étape 1 : Le point de départ – Régression logistique\n",
    "\n",
    "Dans notre exemple, nous avons commencé avec une approche simple : une régression logistique. Pourquoi ? Parce que c’est rapide, facile à implémenter, et souvent suffisant pour valider une idée. Avec cette méthode, nous avons utilisé des caractéristiques textuelles de base, comme la fréquence des mots (bag of words).\n",
    "\n",
    "Résultat :\n",
    "Voici les performances obtenues sur notre dataset :\n",
    "- Précision : 76,5%\n",
    "- F1-score : 74,2%\n",
    "\n",
    "Bien que ce soit un bon point de départ, le modèle manque de finesse pour capturer des relations contextuelles. Par exemple, il peut confondre des phrases comme « pas mauvais » avec des sentiments négatifs.\n",
    "\n",
    "Capture d’écran des résultats et du graphe des erreurs communes.\n",
    "\n",
    "## Étape 2 : Monter en puissance avec des modèles avancés\n",
    "\n",
    "Pour améliorer les résultats, nous avons testé des modèles plus avancés, comme des word embeddings combinés à des réseaux neuronaux (CNN ou RNN). Ces approches permettent de mieux comprendre le contexte des mots en tenant compte des séquences.\n",
    "\n",
    "Résultat avec un CNN :\n",
    "- Précision : 82,7%\n",
    "- F1-score : 81,4%\n",
    "\n",
    "Cette amélioration montre que le modèle capture désormais des nuances plus subtiles dans les sentiments. Cependant, la complexité augmente : le modèle prend plus de temps à entraîner, et le suivi des hyperparamètres devient critique.\n",
    "\n",
    "Capture d’écran des résultats comparés aux erreurs de la régression logistique.\n",
    "\n",
    "## Étape 3 : L’État de l’art avec BERT\n",
    "\n",
    "Enfin, nous avons utilisé BERT, un modèle préentraîné d’État de l’art, capable de comprendre les subtilités linguistiques et les relations complexes dans les phrases. Avec BERT, les résultats ont été impressionnants :\n",
    "\n",
    "Résultat avec BERT :\n",
    "- Précision : 89,5%\n",
    "- F1-score : 88,9%\n",
    "\n",
    "Cependant, l’utilisation de BERT a introduit de nouveaux défis :\n",
    "- Temps d’entraînement prolongé : Il a fallu 10 fois plus de temps que pour la régression logistique.\n",
    "- Infrastructure nécessaire : L’entraînement a nécessité un GPU, et le suivi des versions de données et de modèles est devenu encore plus crucial.\n",
    "\n",
    "Capture d’écran des résultats, avec une visualisation des prédictions BERT sur des exemples complexes.\n",
    "\n",
    "## Passage en production avec MLOps\n",
    "\n",
    "À chaque étape, les modèles deviennent plus performants, mais aussi plus complexes à gérer. C’est ici que le MLOps prend tout son sens. Voici comment il a structuré notre projet :\n",
    "\n",
    "### 1. Suivi des expérimentations\n",
    "\n",
    "Grâce à des outils comme MLflow, nous avons enregistré chaque expérimentation :\n",
    "- Version des données utilisées.\n",
    "- Hyperparamètres et configurations des modèles.\n",
    "- Performances obtenues (précision, F1-score, etc.).\n",
    "\n",
    "Cela nous a permis de comparer les résultats facilement et de justifier nos choix à chaque étape.\n",
    "\n",
    "### 2. Automatisation des pipelines\n",
    "\n",
    "Nous avons automatisé les étapes clés :\n",
    "- Préparation des données (nettoyage, transformation).\n",
    "- Entraînement des modèles avec différentes configurations.\n",
    "- Validation sur des ensembles indépendants.\n",
    "\n",
    "Un outil comme Kubeflow nous a aidés à orchestrer ces tâches et à les rendre reproductibles.\n",
    "\n",
    "### 3. Monitoring en production\n",
    "\n",
    "Une fois BERT déployé, un système de monitoring a été mis en place pour surveiller les performances en temps réel. Par exemple :\n",
    "- Détection des drifts de données : Si les avis clients évoluent (argot, emojis, etc.).\n",
    "- Suivi des métriques : Si le modèle perd en précision ou en rappel.\n",
    "\n",
    "Conclusion : Apprendre, structurer, et industrialiser\n",
    "\n",
    "L’histoire de ce projet d’analyse de sentiments montre bien la double nature des projets ML. D’un côté, il y a l’aspect exploratoire : tester différents modèles et approches pour trouver ce qui fonctionne. De l’autre, il y a le besoin d’industrialisation, où la reproductibilité et la fiabilité deviennent essentielles.\n",
    "\n",
    "Le MLOps, en s’appuyant sur les principes du DevOps, nous a permis de structurer cette transition. Il a transformé une série d’expérimentations en une solution robuste, prête pour la production. Et à chaque étape, il nous a offert des outils pour gérer la complexité croissante du Machine Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p6-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
