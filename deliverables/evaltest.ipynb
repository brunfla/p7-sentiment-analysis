{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions Générales sur la Validation et le Découpage des Données\n",
    "\n",
    "- Pourquoi est-il important de séparer les données en ensembles d'entraînement (train), de validation (val) et de test (test) lors du développement d’un modèle de machine learning ?\n",
    "\n",
    "ALors, il faut toujours séarer train et test, test c'est donc pour évaluer le model sur des données externes, pour avoir un test bien indépendant. Pour les données Val, cela sert principalement à faire de l'optimisation de parametres et du early stopping.\n",
    "\n",
    "- Décris la différence entre la méthode TrainTest simple et la méthode TrainValTest. Dans quel(s) cas privilégierais-tu l’une plutôt que l’autre ?\n",
    "TrainTEst peut etre priviligé si il y a tres peu de données ou s'il n'existe pas d'outil pour optimiser les parametres.\n",
    "\n",
    "- Qu’est-ce que la cross-validation k-fold et pourquoi est-elle particulièrement recommandée pour les petits jeux de données ?\n",
    "La cross-validation est une méthode de validation plus robuste que faire un partion Val statique. EN fait, on divise Train en plusieurs folds, pui on fait la validation sur chaqu'un des fold, et on fait la moyenne, cela permet de répartir \"le calcul\", moins biasiais et on garde toutes les données utilisé pour l'entrainement.\n",
    "\n",
    "- Quel est l’avantage de réaliser une cross-validation interne (sur l’ensemble d'entraînement) avant d’évaluer un modèle final sur un ensemble de test réservé ?\n",
    "J'imagine que cela permet de voir si les deux sont comparables, si il y a un écart très important, on pourrait suspecter la qualité du jeu de test. Mais je ne sais pas bien.\n",
    "\n",
    "- Dans quel(s) contexte(s) envisagerais-tu d’utiliser une validation par Leave-One-Out Cross-Validation (LOOCV) et quelles en sont les principales limites ?\n",
    "C'est quoi ?\n",
    "\n",
    "- Explique le principe de la nested cross-validation et dans quel cas on l’utilise ?\n",
    "C'est quoi\n",
    "\n",
    "- Pour un dataset fortement déséquilibré (par exemple en classification), quelle stratégie de découpage recommanderais-tu pour avoir une meilleure stabilité des estimations de performance ?\n",
    "Une cross validation stratifié\n",
    "\n",
    "- Comment gérer un problème de série temporelle lors de la validation du modèle, et pourquoi une simple division aléatoire des données n’est-elle pas appropriée dans ce cas ?\n",
    "\n",
    "- Qu'est-ce que le bootstrapping, et dans quel(s) contexte(s) cette méthode est-elle particulièrement utile ?\n",
    "\n",
    "- Dans le cadre d’un projet critique, pourquoi réserver un petit pourcentage du dataset pour un test final indépendant peut-il être préférable, même lorsqu’une cross-validation est effectuée sur le reste des données ?\n",
    "\n",
    "### Questions Liées à MLops (Cycle de Vie, Déploiement, Monitoring)\n",
    "\n",
    "- Dans une démarche MLops, à quel moment met-on en place le découpage des données (train/val/test) et pourquoi est-ce crucial pour le pipeline de modélisation ?\n",
    "\n",
    "- Lors d’un processus de CI/CD (Intégration Continue / Déploiement Continu) pour un projet de ML, comment veiller à ce que les données utilisées pour la validation restent représentatives d’un environnement de production qui évolue ?\n",
    "\n",
    "- Quels indicateurs clés (metrics) surveillerais-tu en production pour détecter un éventuel drift (dérive) du modèle ? Comment le découpage des données en validation temporelle peut aider à anticiper ce drift ?\n",
    "\n",
    "- En MLops, que se passe-t-il lorsqu’un nouveau batch de données arrive ? Quels sont les scénarios possibles pour mettre à jour un modèle déjà en production (retraining, réévaluation, etc.) ?\n",
    "\n",
    "- Pourquoi est-il important de documenter rigoureusement le processus de découpage des données (par exemple, aléatoire vs temporel), et quelles conséquences une mauvaise traçabilité pourrait-elle avoir en production ?\n",
    "\n",
    "- Comment intégrer une stratégie de validation continue dans un pipeline MLops automatisé, de sorte à détecter rapidement les dégradations de performances après déploiement ?\n",
    "\n",
    "- Quels sont les risques si l’on réutilise le même ensemble de test à trop de reprises pour valider des versions successives du modèle ? Comment mitiger ces risques ?\n",
    "\n",
    "- Décris comment tu gèrerais la gestion des versions (versioning) des datasets et des modèles dans un projet MLops, et pourquoi cela est essentiel pour la reproductibilité ?\n",
    "\n",
    "### Questions d’Étude de Cas\n",
    "\n",
    "- Etude de cas – Petit Dataset\n",
    "Tu disposes d’un dataset de seulement 500 observations. Le but est de développer un modèle de classification pour un problème de détection de fraude. Comment mettrais-tu en place la validation et le découpage des données pour obtenir une évaluation fiable, sachant que chaque exemple compte beaucoup ?\n",
    "\n",
    "- Étude de cas – Dataset Déséquilibré\n",
    "Tu as un dataset de 50 000 échantillons, mais 90% d’entre eux appartiennent à la classe “non-fraude”. Quels types de découpage et de validation mettrais-tu en place pour garantir que tes métriques (par ex. F1-score, AUC) soient fiables et que la performance soit comparable entre différents essais ?\n",
    "\n",
    "- Étude de cas – Série Temporelle\n",
    "Tu travailles sur une série temporelle de données financières (plusieurs millions d’observations) et tu dois construire un modèle prédictif des cours de bourse. Quelles sont les étapes et bonnes pratiques pour séparer tes données, valider ton modèle et le déployer en production ?\n",
    "\n",
    "- Étude de cas – Projet Critique avec Besoin d’Audit\n",
    "Pour un projet d’assurance, la réglementation impose de conserver un audit complet du processus de construction du modèle. Comment ensures-tu la traçabilité de la stratégie de découpage, de la sélection des hyperparamètres et des résultats de test final dans un cadre MLops ?\n",
    "\n",
    "- Étude de cas – Déploiement en Production avec Mise à Jour Fréquente\n",
    "Tu déploies un système de recommandation, et les données des utilisateurs évoluent fortement (nouvelles préférences, nouveaux produits). Quel protocole de validation pourrais-tu mettre en place pour détecter rapidement la baisse de performance du modèle, et comment organiser le re-training ?\n",
    "\n",
    "Ces questions, à la fois théoriques et pratiques, te permettront d’évaluer et de démontrer :\n",
    "\n",
    "- Ta compréhension des différents schémas de découpage et de validation (y compris leurs avantages et inconvénients).\n",
    "- Tes connaissances des pratiques MLops (cycle de vie de projet, CI/CD, monitoring des performances, gestion du drift, etc.).\n",
    "- Ta capacité à appliquer ces principes à des cas d’usage concrets (datasets déséquilibrés, très petites bases de données, séries temporelles, etc.).\n",
    "\n",
    "N’hésite pas à creuser chaque point et à illustrer tes réponses par des exemples pratiques ou des retours d’expérience si possible. Bonne préparation !%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 :\n",
    "Dans le cadre d’un petit dataset (par exemple quelques centaines d’observations), comment expliquerais-tu le choix entre une cross-validation k-fold et une Leave-One-Out Cross-Validation (LOOCV) pour valider un modèle, et quels sont les facteurs qui pourraient te faire préférer l’une ou l’autre ?\n",
    "\n",
    "Explication / Rappel :\n",
    "- Avec très peu de données, on souhaite souvent maximiser l’usage des données d’entraînement tout en obtenant une estimation robuste des performances.\n",
    "- La Leave-One-Out Cross-Validation (LOOCV) utilise nn folds (où nn est la taille totale du dataset), donc chaque point est utilisé comme test set unique à un moment donné. Cela maximise la quantité de données d’entraînement à chaque itération, mais peut introduire une variance plus importante dans l’estimation de performance et être coûteux en calcul.\n",
    "- La k-fold cross-validation (k=5 ou k=10) donne souvent un compromis acceptable : on entraîne sur k−1kkk−1​ du dataset et on teste sur 1kk1​. Cela reste moins coûteux que LOOCV et peut réduire la variance de l’estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
