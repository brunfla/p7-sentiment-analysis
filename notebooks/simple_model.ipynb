{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle simple : Régression Logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel de la problématique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire\n",
    "\n",
    "- **Étape 1 : Initialisation des paramètres**\n",
    "  - Charger la configuration (Hydra)\n",
    "  - Charger le dataset\n",
    "  - Charger MLflow\n",
    "\n",
    "- **Étape 2 : Prétraitement**\n",
    "  - Suppression des tweets vides\n",
    "  - Vectorisation TF-IDF\n",
    "\n",
    "- **Étape 3 : Entraînement et Validation**\n",
    "  - Découpage des données (`dataSplitting`)\n",
    "  - **Optimisation des hyperparamètres** : deux options disponibles :\n",
    "    - **Optuna** : Recherche bayésienne pour une exploration efficace.\n",
    "    - **GridSearch** : Recherche exhaustive sur une grille prédéfinie.\n",
    "  - Entraînement final\n",
    "  - Validation sur un ensemble de validation\n",
    "\n",
    "- **Étape 4 : Évaluation Finale et Comparaison**\n",
    "  - Évaluation sur un ensemble de test indépendant\n",
    "  - Enregistrement du run dans MLflow (métriques et artefacts)\n",
    "  - Comparaison avec d'autres modèles enregistrés dans le **Model Registry**\n",
    "\n",
    "- **Étape 5 : Promotion et Validation en Production**\n",
    "  - Promotion du modèle validé en `Staging` dans MLflow\n",
    "  - Tests supplémentaires pour validation finale :\n",
    "    - Tests d'inférence sur des cas d'usage réels\n",
    "    - Validation d'intégration dans l'environnement cible\n",
    "  - Promotion vers le stage `Production` dans MLflow après validation\n",
    "  - **Configuration de la surveillance en production** :\n",
    "    - Utilisation d'Azure Application Insights pour le monitoring\n",
    "    - Ajout de métriques clés spécifiques au projet d'analyse de sentiment :\n",
    "      - **Dérive des données** : Détection des changements dans la distribution des entrées.\n",
    "      - **Temps d'inférence** : Suivi des performances en temps réel.\n",
    "      - **Précision par classe** : Surveillance des variations pour des catégories spécifiques.\n",
    "\n",
    "#### Critères de Promotion vers `Staging` et `Production`\n",
    "\n",
    "| **Étape**       | **Critère**                                                                                                  | **Seuil**                 | **Commentaires**                                                 |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------|--------------------------------------|------------------------------------------------------------------|\n",
    "| **Validation**   | Performances sur l'ensemble de validation.                                                                 | Accuracy ≥ 85%                       | Peut varier selon la nature du projet (par ex., `F1-score` > 0.8). |\n",
    "| **Test Final**   | Performances sur l'ensemble Test indépendant.                                                              | F1-Score ≥ 80%                       | Doit garantir une bonne généralisation.                          |\n",
    "| **Monitoring**   | Aucun drift détecté sur les données d'entrée dans Azure Insights.                                           | KS Test p-value ≥ 0.05               | Test de dérive des données.                                      |\n",
    "| **Staging**      | Validation des cas d'usage réels dans l'environnement cible.                                                | ≥ 90% de succès sur les tests réels. | Exemples représentatifs des données réelles.                     |\n",
    "| **Production**   | Aucune régression détectée dans les métriques critiques après une période de tests en `Staging`.            | Temps d'inférence < 300ms.           | Important pour l'intégration en temps réel.                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratégies globales adaptées au pipeline MLOps\n",
    "\n",
    "| **Étape/Stratégie globale**                 | **trainTest**                                                                                     | **trainValTest**                                                                                     | **crossValidation**                                                                                           |\n",
    "|---------------------------------------------|--------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|\n",
    "| **Découpage des Données**                   | - 2 ensembles : **Train** et **Test**.                                                          | - 3 ensembles : **Train**, **Validation**, et **Test**.                                             | - Pas d’ensembles fixes : les données sont divisées en `k` folds pour validation croisée.                     |\n",
    "| **Cas d'usage**                             | - Exploration rapide : tester une hypothèse ou une nouvelle feature.                            | - Comparaison et optimisation de modèles avec une validation explicite.                             | - Lorsque les données sont limitées ou fortement hétérogènes.                                                |\n",
    "| **Optimisation des Hyperparamètres**         | - Pas possible sans `Validation`.                                                               | - Optimisation sur l'ensemble **Validation**.                                                       | - Validation croisée : optimisation intégrée via les splits.                                                 |\n",
    "| **Entraînement Final**                       | - Sur l'ensemble **Train** avec des hyperparamètres par défaut ou optimisés (si manuel).         | - Sur l'ensemble **Train** avec les hyperparamètres optimaux.                                        | - Souvent sur toutes les données de `Train`, car validation croisée optimise sur tous les splits.             |\n",
    "| **Validation (sur un ensemble dédié)**       | - Non applicable (ou via un plugin comme `crossValidation` sur `Train`).                        | - Sur l'ensemble **Validation**, métriques loguées.                                                 | - Validation incluse dans la validation croisée (k-folds).                                                   |\n",
    "| **Évaluation Finale et Comparaison**         | - Sur l'ensemble **Test**, métriques loguées dans MLflow.                                        | - Sur l'ensemble **Test**, métriques loguées dans MLflow.                                           | - Peut nécessiter un ensemble **Test** séparé pour l’évaluation finale.                                       |\n",
    "| **Cas d'usage spécifique**                   | - Prototypage rapide ou tests exploratoires.                                                    | - Standard pour des pipelines structurés, adaptés aux projets de production.                        | - Évaluation robuste sur plusieurs splits pour réduire les risques de biais.                                 |\n",
    "| **Enregistrement dans MLflow**              | - Run enregistré pour l'évaluation finale uniquement.                                            | - Run enregistré après chaque étape (Validation, Test).                                             | - Run enregistré après validation croisée et évaluation finale (si Test séparé).                              |\n",
    "| **Promotion et Validation en Production**    | - Promotion limitée (pas de validation explicite, risques plus élevés en production).            | - Modèle validé en `Staging`, testé et promu vers `Production`.                                      | - Promotion possible après validation croisée robuste et éventuelle validation finale sur un ensemble Test.   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1 : Initialisation des paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charger la configuration (Hydra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration globale :\n",
      "{'dataset': {'path': './output/data_clean.csv'}, 'strategy': {'_target_': 'trainValTest', 'testSize': 0.2, 'validationSize': 0.25, 'randomSeed': 42}, 'model': {'name': 'logistic_regression_model', 'version': '1.0', 'parameters': {'solver': 'liblinear', 'penalty': 'l2', 'C': 1.0}, 'mlflow': {'trackingUri': 'http://127.0.0.1:5000', 'experiment': {'name': 'p7-sentiment-analysis', 'run': {'name': 'logistic_regression_run', 'description': 'Training with logistic regression', 'tags': {'modelType': 'logistic_regression', 'datasetVersion': 'v1.0'}}}}}, 'vectorizer': {'_target_': 'tfidfVectorizer', 'stopWords': 'english', 'maxFeatures': 1000, 'ngramRange': [1, 2]}, 'hyperparameterOptimization': {'_target_': 'gridSearch', 'enabled': True, 'crossValidationFolds': 5, 'verbosityLevel': 1, 'parallelJobs': -1, 'paramGrid': {'penalty': ['l1', 'l2'], 'C': [0.1, 1, 10], 'solver': ['liblinear', 'saga'], 'max_iter': [1000, 2000]}}, 'validation': {'enabled': False}}\n",
      "\n",
      "Modèle sélectionné : logistic_regression_model\n",
      "Paramètres du modèle : {'solver': 'liblinear', 'penalty': 'l2', 'C': 1.0}\n",
      "\n",
      "Dataset chargé avec 1600000 lignes et 7 colonnes.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from omegaconf import DictConfig\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "# Réinitialiser Hydra si déjà initialisé\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "\n",
    "# Initialiser Hydra avec une nouvelle configuration\n",
    "initialize(config_path=\"config\", version_base=None)\n",
    "cfg = compose(config_name=\"config\")\n",
    "\n",
    "# Afficher la configuration globale\n",
    "print(\"Configuration globale :\")\n",
    "print(cfg)\n",
    "\n",
    "# Charger les paramètres du modèle\n",
    "model_config = cfg.model\n",
    "print(f\"\\nModèle sélectionné : {model_config.name}\")\n",
    "print(f\"Paramètres du modèle : {model_config.parameters}\")\n",
    "\n",
    "# Charger le dataset\n",
    "dataset_path = cfg.dataset.path  # Utiliser la clé correcte\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(f\"\\nDataset chargé avec {len(df)} lignes et {len(df.columns)} colonnes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charger MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow server is already running on http://127.0.0.1:5000.\n",
      "Démarrer un run.\n",
      "MLflow run started.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import mlflow\n",
    "from omegaconf import DictConfig\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "\n",
    "def is_mlflow_running(host=\"127.0.0.1\", port=5000):\n",
    "    \"\"\"\n",
    "    Vérifie si le serveur MLflow est en cours d'exécution.\n",
    "    \"\"\"\n",
    "    url = f\"http://{host}:{port}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        return response.status_code == 200  # Vérifie le code de réponse HTTP\n",
    "    except requests.ConnectionError:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Vérifier si MLflow est en cours d'exécution\n",
    "mlflow_host = cfg.model.mlflow.trackingUri.split(\"://\")[1].split(\":\")[0]\n",
    "mlflow_port = int(cfg.model.mlflow.trackingUri.split(\":\")[-1])\n",
    "\n",
    "if not is_mlflow_running(host=mlflow_host, port=mlflow_port):\n",
    "    subprocess.Popen([\"mlflow\", \"server\", \"--host\", mlflow_host, \"--port\", str(mlflow_port)])\n",
    "    print(f\"MLflow server started on http://{mlflow_host}:{mlflow_port}.\")\n",
    "else:\n",
    "    print(f\"MLflow server is already running on http://{mlflow_host}:{mlflow_port}.\")\n",
    "    # Vérifier si un run est déjà actif\n",
    "    if mlflow.active_run() is not None:\n",
    "        print(f\"Ending the active run with ID: {mlflow.active_run().info.run_id}\")\n",
    "        mlflow.end_run()\n",
    "\n",
    "# Configurer MLflow\n",
    "print(\"Démarrer un run.\")\n",
    "mlflow.set_tracking_uri(cfg.model.mlflow.trackingUri)\n",
    "mlflow.set_experiment(cfg.model.mlflow.experiment.name)\n",
    "mlflow.start_run(run_name=cfg.model.mlflow.experiment.run.name)\n",
    "print(\"MLflow run started.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2 : Prétraitement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression des lignes vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des lignes vides\n",
    "df[df[\"tweet\"].isna() | (df[\"tweet\"] == \"\")]\n",
    "df = df[~(df['tweet'].isna() | (df['tweet'] == \"\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorisation terminée avec 1000 caractéristiques.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vérifier le type de vectorizer configuré\n",
    "if cfg.vectorizer._target_ == \"tfidfVectorizer\":\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=cfg.vectorizer.stopWords,\n",
    "        max_features=cfg.vectorizer.maxFeatures,\n",
    "        ngram_range=tuple(cfg.vectorizer.ngramRange)\n",
    "    )\n",
    "else:\n",
    "    raise KeyError(\"La configuration 'tfidVectorizer' est absente ou mal définie dans 'vectorizer'.\")\n",
    "    \n",
    "# Appliquer fit_transform sur les tweets\n",
    "X = vectorizer.fit_transform(df['tweet'])\n",
    "\n",
    "print(f\"TF-IDF vectorisation terminée avec {X.shape[1]} caractéristiques.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3 : Entraînement et Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Découpage des données (`dataSplitting`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données découpées avec la stratégie 'trainValTest':\n",
      "Train: (957936, 1000), Validation: (319312, 1000), Test: (319313, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Encodage binaire de la cible\n",
    "y = df['id'].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "# Vérifier la stratégie sélectionnée\n",
    "if cfg.strategy._target_ == \"trainValTest\":\n",
    "    # Charger les paramètres de la stratégie\n",
    "    params = cfg.strategy\n",
    "    # Découpage Train/Test\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=params.testSize, random_state=params.randomSeed\n",
    "    )\n",
    "    # Découpage Train/Validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=params.validationSize, random_state=params.randomSeed\n",
    "    )\n",
    "    print(f\"Données découpées avec la stratégie 'trainValTest':\")\n",
    "    print(f\"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "elif cfg.strategy._target_ == \"trainTest\":\n",
    "    # Charger les paramètres de la stratégie\n",
    "    params = cfg.strategy\n",
    "    # Découpage Train/Test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=params.testSize, random_state=params.randomSeed\n",
    "    )\n",
    "    X_val, y_val = None, None  # Pas de validation pour cette stratégie\n",
    "    print(f\"Données découpées avec la stratégie 'trainTest':\")\n",
    "    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "elif cfg.strategy._target_ == \"crossValidation\":\n",
    "    # Charger les paramètres de la stratégie\n",
    "    params = cfg.strategy\n",
    "    kfold = KFold(n_splits=params.folds, shuffle=True, random_state=params.randomSeed)\n",
    "    folds = list(kfold.split(X, y))\n",
    "    print(f\"Données découpées avec la stratégie 'crossValidation':\")\n",
    "    print(f\"Nombre de folds: {len(folds)}\")\n",
    "    # Exemple d'accès au premier fold\n",
    "    train_idx, val_idx = folds[0]\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    X_test, y_test = None, None  # Pas de test explicite pour cette stratégie\n",
    "    print(f\"Premier fold - Train: {X_train.shape}, Validation: {X_val.shape}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Stratégie de découpage des données '{cfg.strategy._target_}' non reconnue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation des Hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Méthodes d'optimisation :\n",
    "    - GridSearchCV :\n",
    "        - Explore toutes les combinaisons d’hyperparamètres définis dans une grille.\n",
    "        - Utilise la validation croisée pour évaluer chaque combinaison.\n",
    "        - Retourne :\n",
    "            - Les meilleurs hyperparamètres (best_params_).\n",
    "            - Le modèle entraîné avec ces hyperparamètres (best_estimator_).\n",
    "\n",
    "    - Optuna :\n",
    "        - Optimisation bayésienne avec espace de recherche dynamique.\n",
    "        - Fonction objectif :\n",
    "            - Entraîne un modèle avec des hyperparamètres suggérés.\n",
    "            - Évalue la performance via validation croisée (cross_val_score).\n",
    "        - Retourne :\n",
    "            - Les meilleurs hyperparamètres (best_params).\n",
    "            - Un modèle final configuré avec ces paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 24 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters Found by GridSearch: {'C': 0.1, 'max_iter': 2000, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test Accuracy: 0.7389677213267233\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import optuna\n",
    "\n",
    "# Paramètres pour l'optimisation\n",
    "if cfg.hyperparameterOptimization._target_ == \"gridSearch\":\n",
    "    param_grid = dict(cfg.hyperparameterOptimization.paramGrid)\n",
    "\n",
    "    # Définir les folds ou l'ensemble de validation\n",
    "    cv = (\n",
    "        cfg.validation.crossValidation.folds\n",
    "        if cfg.strategy._target_ == \"crossValidation\"\n",
    "        else [(list(range(X_train.shape[0])), list(range(X_val.shape[0])))]\n",
    "    )\n",
    "\n",
    "    # Création de GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=LogisticRegression(),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        verbose=cfg.hyperparameterOptimization.verbosityLevel,\n",
    "        n_jobs=cfg.hyperparameterOptimization.parallelJobs,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Récupération des meilleurs paramètres\n",
    "    best_params = grid_search.best_params_\n",
    "    model = grid_search.best_estimator_\n",
    "    print(f\"Best Parameters Found by GridSearch: {best_params}\")\n",
    "\n",
    "    # Évaluation finale sur l'ensemble de test\n",
    "    if X_test is not None and y_test is not None:\n",
    "        test_score = model.score(X_test, y_test)\n",
    "        print(f\"Test Accuracy: {test_score}\")\n",
    "\n",
    "elif cfg.hyperparameterOptimization._target_ == \"optuna\":\n",
    "    def objective(trial):\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
    "        C = trial.suggest_float(\"C\", 0.1, 10, log=True)\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"liblinear\", \"saga\"])\n",
    "        max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "\n",
    "        model = LogisticRegression(penalty=penalty, C=C, solver=solver, max_iter=max_iter)\n",
    "\n",
    "        if cfg.strategy._target_ == \"crossValidation\":\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=cfg.validation.crossValidation.folds)\n",
    "            return scores.mean()\n",
    "        elif cfg.strategy._target_ == \"trainValTest\":\n",
    "            model.fit(X_train, y_train)\n",
    "            return model.score(X_val, y_val)\n",
    "\n",
    "    # Lancer l'optimisation avec Optuna\n",
    "    study = optuna.create_study(direction=cfg.hyperparameterOptimization.optuna.optimizationDirection)\n",
    "    study.optimize(objective, n_trials=cfg.hyperparameterOptimization.optuna.trialCount, timeout=cfg.hyperparameterOptimization.optuna.timeLimitSeconds)\n",
    "\n",
    "    # Récupération des meilleurs paramètres\n",
    "    best_params = study.best_params\n",
    "    model = LogisticRegression(**best_params)\n",
    "    print(f\"Best Parameters Found by Optuna: {best_params}\")\n",
    "\n",
    "    # Évaluation finale sur l'ensemble de test\n",
    "    if X_test is not None and y_test is not None:\n",
    "        model.fit(X_train, y_train)\n",
    "        test_score = model.score(X_test, y_test)\n",
    "        print(f\"Test Accuracy: {test_score}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unsupported hyperparameter optimization method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 4 : Évaluation Finale et Comparaison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Évaluation sur un ensemble de test indépendant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Étape 4 : Évaluation Finale ###\n",
      "Accuracy on Test Set: 0.7390\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.69      0.73    159233\n",
      "           1       0.72      0.78      0.75    160080\n",
      "\n",
      "    accuracy                           0.74    319313\n",
      "   macro avg       0.74      0.74      0.74    319313\n",
      "weighted avg       0.74      0.74      0.74    319313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Étape 4 : Évaluation Finale et Comparaison\n",
    "if X_test is not None and y_test is not None:\n",
    "    print(\"\\n### Étape 4 : Évaluation Finale ###\")\n",
    "    \n",
    "    # Évaluer le modèle sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "else:\n",
    "    print(\"\\n### Étape 4 : Évaluation Finale ###\")\n",
    "    print(\"Aucun ensemble de test indépendant disponible pour évaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enregistrement du run dans MLflow (métriques et artefacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enregistrement du run dans MLflow (métriques et artefacts)\n",
    "def log_run_metrics_and_artifacts(model, cfg, val_accuracy, val_f1, y_val=None, y_val_proba=None):\n",
    "    # Extraire les labels depuis la configuration Hydra\n",
    "    hydra_labels = {\n",
    "        \"data_split\": cfg.strategy._target_,\n",
    "        \"optimizer\": cfg.hyperparameterOptimization._target_,\n",
    "        \"validation\": cfg.validation.crossValidation.folds if cfg.strategy._target_ == \"crossValidation\" else \"N/A\",\n",
    "        \"experiment_name\": cfg.model.mlflow.experiment.name,\n",
    "        \"run_name\": cfg.model.mlflow.experiment.run.name,\n",
    "    }\n",
    "\n",
    "    # Log tags pour les labels Hydra\n",
    "    for key, value in hydra_labels.items():\n",
    "        mlflow.set_tag(key, value)\n",
    "\n",
    "    # Log des métriques dans MLflow\n",
    "    mlflow.log_metric(\"validation_accuracy\", val_accuracy)\n",
    "    mlflow.log_metric(\"validation_f1_score\", val_f1)\n",
    "\n",
    "    # Enregistrer le rapport de classification dans les logs\n",
    "    if y_val is not None:\n",
    "        val_classification_report = classification_report(y_val, model.predict(y_val))\n",
    "        with open(\"classification_report_val.txt\", \"w\") as f:\n",
    "            f.write(val_classification_report)\n",
    "        mlflow.log_artifact(\"classification_report_val.txt\")\n",
    "        os.remove(\"classification_report_val.txt\")\n",
    "\n",
    "    # Courbe ROC (si disponible)\n",
    "    if y_val_proba is not None:\n",
    "        val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "        mlflow.log_metric(\"validation_roc_auc\", val_roc_auc)\n",
    "\n",
    "        # Générer la courbe ROC\n",
    "        fpr, tpr, _ = roc_curve(y_val, y_val_proba)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {val_roc_auc:.2f})\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid()\n",
    "\n",
    "        # Enregistrer la courbe ROC\n",
    "        roc_curve_path = \"roc_curve_validation.png\"\n",
    "        plt.savefig(roc_curve_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(roc_curve_path)\n",
    "        os.remove(roc_curve_path)\n",
    "\n",
    "    # Enregistrement du modèle dans MLflow\n",
    "    mlflow.log_param(\"model_name\", cfg.model.mlflow.model.name)\n",
    "    mlflow.sklearn.log_model(model, cfg.model.mlflow.model.name)\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "    if y_val_proba is not None:\n",
    "        print(f\"Validation ROC AUC: {val_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run logistic_regression_run at: http://127.0.0.1:5000/#/experiments/277281536415448661/runs/5926d1d65e314b4db3e1f6deec1bbdf9\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/277281536415448661\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison avec d'autres modèles enregistrés dans le **Model Registry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLe meilleur modèle enregistré est \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) avec une validation accuracy de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Appel de la fonction pour comparer le modèle actuel\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m compare_with_registered_models(cfg, \u001b[43mval_accuracy\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Comparaison avec d'autres modèles enregistrés dans le Model Registry\n",
    "def compare_with_registered_models(cfg, val_accuracy):\n",
    "    client = MlflowClient(tracking_uri=cfg.model.mlflow.trackingUri)\n",
    "\n",
    "    # Récupérer tous les modèles enregistrés dans l'expérience\n",
    "    registered_models = client.search_registered_models()\n",
    "    \n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    print(\"\\nComparaison avec les modèles enregistrés dans le Model Registry:\")\n",
    "    for model in registered_models:\n",
    "        model_name = model.name\n",
    "\n",
    "        # Récupérer les versions du modèle\n",
    "        for version in client.get_latest_versions(model_name):\n",
    "            if \"validation_accuracy\" in version.tags:\n",
    "                model_accuracy = float(version.tags[\"validation_accuracy\"])\n",
    "\n",
    "                print(f\"Modèle: {model_name}, Version: {version.version}, Validation Accuracy: {model_accuracy}\")\n",
    "\n",
    "                # Comparer les scores\n",
    "                if model_accuracy > best_accuracy:\n",
    "                    best_accuracy = model_accuracy\n",
    "                    best_model = (model_name, version.version)\n",
    "\n",
    "    print(\"\\nRésultats de la comparaison:\")\n",
    "    if val_accuracy > best_accuracy:\n",
    "        print(f\"Le modèle actuel est le meilleur avec une validation accuracy de {val_accuracy:.4f}.\")\n",
    "    else:\n",
    "        print(f\"Le meilleur modèle enregistré est '{best_model[0]}' (version {best_model[1]}) avec une validation accuracy de {best_accuracy:.4f}.\")\n",
    "\n",
    "# Appel de la fonction pour comparer le modèle actuel\n",
    "compare_with_registered_models(cfg, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 5 : Promotion et Validation en Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import onnxruntime as ort\n",
    "from azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "\n",
    "# Configuration Azure Application Insights\n",
    "exporter = AzureMonitorTraceExporter(connection_string=\"InstrumentationKey=85b10953-35ac-45bc-9192-6044192484fe\")\n",
    "tracer_provider = TracerProvider()\n",
    "span_processor = BatchSpanProcessor(exporter)\n",
    "tracer_provider.add_span_processor(span_processor)\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "# Charger le modèle ONNX\n",
    "onnx_session = ort.InferenceSession('./output/simple_model.onnx')\n",
    "\n",
    "def predict_sentiment(tweet):\n",
    "    \"\"\"Prédire le sentiment d'un tweet avec le modèle ONNX.\"\"\"\n",
    "    inputs = {onnx_session.get_inputs()[0].name: [[tweet]]}  # Respect du format attendu\n",
    "    outputs = onnx_session.run(None, inputs)\n",
    "    predicted_label = outputs[0][0]  # 1 pour \"positif\", 0 pour \"négatif\"\n",
    "    return \"Positif\" if predicted_label == 1 else \"Négatif\"\n",
    "\n",
    "def main():\n",
    "    print(\"=== Test de Sentiment ===\")\n",
    "    while True:\n",
    "        # Saisir un tweet\n",
    "        tweet = input(\"Entrez un tweet (ou tapez 'exit' pour quitter) : \")\n",
    "        if tweet.lower() == 'exit':\n",
    "            print(\"Au revoir !\")\n",
    "            break\n",
    "        \n",
    "        # Prédire le sentiment\n",
    "        sentiment = predict_sentiment(tweet)\n",
    "        print(f\"Prédiction : {sentiment}\")\n",
    "        \n",
    "        # Demander validation utilisateur\n",
    "        validation = input(\"La prédiction est-elle correcte ? (oui/non) : \").strip().lower()\n",
    "        \n",
    "        if validation == \"non\":\n",
    "            print(\"Merci pour votre retour, une trace a été envoyée.\")\n",
    "            \n",
    "            # Envoyer une trace à Application Insights\n",
    "            with tracer.start_as_current_span(\"Validation incorrecte\"):\n",
    "                span = trace.get_current_span()\n",
    "                span.set_attribute(\"tweet\", tweet)\n",
    "                span.set_attribute(\"prédiction\", sentiment)\n",
    "                span.set_attribute(\"validation\", \"Non\")\n",
    "        elif validation == \"oui\":\n",
    "            print(\"Merci pour votre validation.\")\n",
    "        else:\n",
    "            print(\"Réponse invalide. Veuillez entrer 'oui' ou 'non'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enregistrement dans le Model Registry "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "model_uri = f\"runs:/{run.info.run_id}/{cfg.mlflow.model.name}\"\n",
    "\n",
    "try:\n",
    "    # Vérifier si le modèle existe dans le registre\n",
    "    client.get_registered_model(cfg.mlflow.model.name)\n",
    "except mlflow.exceptions.MlflowException:\n",
    "    # Créer un modèle dans le registre s'il n'existe pas\n",
    "    client.create_registered_model(cfg.mlflow.model.name)\n",
    "\n",
    "# Créer une nouvelle version du modèle dans le registre\n",
    "client.create_model_version(\n",
    "    name=cfg.mlflow.model.name,\n",
    "    source=model_uri,\n",
    "    run_id=run.info.run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "\n",
    "# Transitionner une version de modèle vers Production\n",
    "client.transition_model_version_stage(\n",
    "    name=cfg.mlflow.model.name,\n",
    "    version=1,  # La version à transitionner\n",
    "    stage=\"Production\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p6-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
