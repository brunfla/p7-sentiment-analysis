{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod√®le simple : R√©gression Logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel de la probl√©matique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire\n",
    "\n",
    "- **√âtape 1 : Initialisation des param√®tres**\n",
    "  - Charger la configuration (Hydra)\n",
    "  - Charger le dataset\n",
    "  - Charger MLflow\n",
    "\n",
    "- **√âtape 2 : Pr√©traitement**\n",
    "  - Suppression des tweets vides\n",
    "  - Vectorisation TF-IDF\n",
    "\n",
    "- **√âtape 3 : Entra√Ænement et Validation**\n",
    "  - D√©coupage des donn√©es (`dataSplitting`)\n",
    "  - **Optimisation des hyperparam√®tres** : deux options disponibles :\n",
    "    - **Optuna** : Recherche bay√©sienne pour une exploration efficace.\n",
    "    - **GridSearch** : Recherche exhaustive sur une grille pr√©d√©finie.\n",
    "  - Entra√Ænement final\n",
    "  - Validation sur un ensemble de validation\n",
    "\n",
    "- **√âtape 4 : √âvaluation Finale et Comparaison**\n",
    "  - √âvaluation sur un ensemble de test ind√©pendant\n",
    "  - Enregistrement du run dans MLflow (m√©triques et artefacts)\n",
    "  - Comparaison avec d'autres mod√®les enregistr√©s dans le **Model Registry**\n",
    "\n",
    "- **√âtape 5 : Promotion et Validation en Production**\n",
    "  - Promotion du mod√®le valid√© en `Staging` dans MLflow\n",
    "  - Tests suppl√©mentaires pour validation finale :\n",
    "    - Tests d'inf√©rence sur des cas d'usage r√©els\n",
    "    - Validation d'int√©gration dans l'environnement cible\n",
    "  - Promotion vers le stage `Production` dans MLflow apr√®s validation\n",
    "  - **Configuration de la surveillance en production** :\n",
    "    - Utilisation d'Azure Application Insights pour le monitoring\n",
    "    - Ajout de m√©triques cl√©s sp√©cifiques au projet d'analyse de sentiment :\n",
    "      - **D√©rive des donn√©es** : D√©tection des changements dans la distribution des entr√©es.\n",
    "      - **Temps d'inf√©rence** : Suivi des performances en temps r√©el.\n",
    "      - **Pr√©cision par classe** : Surveillance des variations pour des cat√©gories sp√©cifiques.\n",
    "\n",
    "#### Crit√®res de Promotion vers `Staging` et `Production`\n",
    "\n",
    "| **√âtape**       | **Crit√®re**                                                                                                  | **Seuil**                 | **Commentaires**                                                 |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------|--------------------------------------|------------------------------------------------------------------|\n",
    "| **Validation**   | Performances sur l'ensemble de validation.                                                                 | Accuracy ‚â• 85%                       | Peut varier selon la nature du projet (par ex., `F1-score` > 0.8). |\n",
    "| **Test Final**   | Performances sur l'ensemble Test ind√©pendant.                                                              | F1-Score ‚â• 80%                       | Doit garantir une bonne g√©n√©ralisation.                          |\n",
    "| **Monitoring**   | Aucun drift d√©tect√© sur les donn√©es d'entr√©e dans Azure Insights.                                           | KS Test p-value ‚â• 0.05               | Test de d√©rive des donn√©es.                                      |\n",
    "| **Staging**      | Validation des cas d'usage r√©els dans l'environnement cible.                                                | ‚â• 90% de succ√®s sur les tests r√©els. | Exemples repr√©sentatifs des donn√©es r√©elles.                     |\n",
    "| **Production**   | Aucune r√©gression d√©tect√©e dans les m√©triques critiques apr√®s une p√©riode de tests en `Staging`.            | Temps d'inf√©rence < 300ms.           | Important pour l'int√©gration en temps r√©el.                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strat√©gies globales adapt√©es au pipeline MLOps\n",
    "\n",
    "| **√âtape/Strat√©gie globale**                 | **trainTest**                                                                                     | **trainValTest**                                                                                     | **crossValidation**                                                                                           |\n",
    "|---------------------------------------------|--------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|\n",
    "| **D√©coupage des Donn√©es**                   | - 2 ensembles : **Train** et **Test**.                                                          | - 3 ensembles : **Train**, **Validation**, et **Test**.                                             | - Pas d‚Äôensembles fixes‚ÄØ: les donn√©es sont divis√©es en `k` folds pour validation crois√©e.                     |\n",
    "| **Cas d'usage**                             | - Exploration rapide : tester une hypoth√®se ou une nouvelle feature.                            | - Comparaison et optimisation de mod√®les avec une validation explicite.                             | - Lorsque les donn√©es sont limit√©es ou fortement h√©t√©rog√®nes.                                                |\n",
    "| **Optimisation des Hyperparam√®tres**         | - Pas possible sans `Validation`.                                                               | - Optimisation sur l'ensemble **Validation**.                                                       | - Validation crois√©e‚ÄØ: optimisation int√©gr√©e via les splits.                                                 |\n",
    "| **Entra√Ænement Final**                       | - Sur l'ensemble **Train** avec des hyperparam√®tres par d√©faut ou optimis√©s (si manuel).         | - Sur l'ensemble **Train** avec les hyperparam√®tres optimaux.                                        | - Souvent sur toutes les donn√©es de `Train`, car validation crois√©e optimise sur tous les splits.             |\n",
    "| **Validation (sur un ensemble d√©di√©)**       | - Non applicable (ou via un plugin comme `crossValidation` sur `Train`).                        | - Sur l'ensemble **Validation**, m√©triques logu√©es.                                                 | - Validation incluse dans la validation crois√©e (k-folds).                                                   |\n",
    "| **√âvaluation Finale et Comparaison**         | - Sur l'ensemble **Test**, m√©triques logu√©es dans MLflow.                                        | - Sur l'ensemble **Test**, m√©triques logu√©es dans MLflow.                                           | - Peut n√©cessiter un ensemble **Test** s√©par√© pour l‚Äô√©valuation finale.                                       |\n",
    "| **Cas d'usage sp√©cifique**                   | - Prototypage rapide ou tests exploratoires.                                                    | - Standard pour des pipelines structur√©s, adapt√©s aux projets de production.                        | - √âvaluation robuste sur plusieurs splits pour r√©duire les risques de biais.                                 |\n",
    "| **Enregistrement dans MLflow**              | - Run enregistr√© pour l'√©valuation finale uniquement.                                            | - Run enregistr√© apr√®s chaque √©tape (Validation, Test).                                             | - Run enregistr√© apr√®s validation crois√©e et √©valuation finale (si Test s√©par√©).                              |\n",
    "| **Promotion et Validation en Production**    | - Promotion limit√©e (pas de validation explicite, risques plus √©lev√©s en production).            | - Mod√®le valid√© en `Staging`, test√© et promu vers `Production`.                                      | - Promotion possible apr√®s validation crois√©e robuste et √©ventuelle validation finale sur un ensemble Test.   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1 : Initialisation des param√®tres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charger la configuration (Hydra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration globale :\n",
      "{'dataset': {'path': './output/data_clean.csv'}, 'strategy': {'_target_': 'trainValTest', 'testSize': 0.2, 'validationSize': 0.25, 'randomSeed': 42}, 'model': {'name': 'logistic_regression_model', 'version': '1.0', 'parameters': {'solver': 'liblinear', 'penalty': 'l2', 'C': 1.0}, 'mlflow': {'trackingUri': 'http://127.0.0.1:5000', 'experiment': {'name': 'p7-sentiment-analysis', 'run': {'name': 'logistic_regression_run', 'description': 'Training with logistic regression', 'tags': {'modelType': 'logistic_regression', 'datasetVersion': 'v1.0'}}}}}, 'vectorizer': {'_target_': 'tfidfVectorizer', 'stopWords': 'english', 'maxFeatures': 1000, 'ngramRange': [1, 2]}, 'hyperparameterOptimization': {'_target_': 'gridSearch', 'enabled': True, 'crossValidationFolds': 5, 'verbosityLevel': 1, 'parallelJobs': -1, 'paramGrid': {'penalty': ['l1', 'l2'], 'C': [0.1, 1, 10], 'solver': ['liblinear', 'saga'], 'max_iter': [1000, 2000]}}, 'validation': {'enabled': False}}\n",
      "\n",
      "Mod√®le s√©lectionn√© : logistic_regression_model\n",
      "Param√®tres du mod√®le : {'solver': 'liblinear', 'penalty': 'l2', 'C': 1.0}\n",
      "\n",
      "Dataset charg√© avec 1600000 lignes et 7 colonnes.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from omegaconf import DictConfig\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "# R√©initialiser Hydra si d√©j√† initialis√©\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "\n",
    "# Initialiser Hydra avec une nouvelle configuration\n",
    "initialize(config_path=\"config\", version_base=None)\n",
    "cfg = compose(config_name=\"config\")\n",
    "\n",
    "# Afficher la configuration globale\n",
    "print(\"Configuration globale :\")\n",
    "print(cfg)\n",
    "\n",
    "# Charger les param√®tres du mod√®le\n",
    "model_config = cfg.model\n",
    "print(f\"\\nMod√®le s√©lectionn√© : {model_config.name}\")\n",
    "print(f\"Param√®tres du mod√®le : {model_config.parameters}\")\n",
    "\n",
    "# Charger le dataset\n",
    "dataset_path = cfg.dataset.path  # Utiliser la cl√© correcte\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(f\"\\nDataset charg√© avec {len(df)} lignes et {len(df.columns)} colonnes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charger MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow server is already running on http://127.0.0.1:5000.\n",
      "D√©marrer un run.\n",
      "MLflow run started.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import mlflow\n",
    "from omegaconf import DictConfig\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "\n",
    "def is_mlflow_running(host=\"127.0.0.1\", port=5000):\n",
    "    \"\"\"\n",
    "    V√©rifie si le serveur MLflow est en cours d'ex√©cution.\n",
    "    \"\"\"\n",
    "    url = f\"http://{host}:{port}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        return response.status_code == 200  # V√©rifie le code de r√©ponse HTTP\n",
    "    except requests.ConnectionError:\n",
    "        return False\n",
    "\n",
    "\n",
    "# V√©rifier si MLflow est en cours d'ex√©cution\n",
    "mlflow_host = cfg.model.mlflow.trackingUri.split(\"://\")[1].split(\":\")[0]\n",
    "mlflow_port = int(cfg.model.mlflow.trackingUri.split(\":\")[-1])\n",
    "\n",
    "if not is_mlflow_running(host=mlflow_host, port=mlflow_port):\n",
    "    subprocess.Popen([\"mlflow\", \"server\", \"--host\", mlflow_host, \"--port\", str(mlflow_port)])\n",
    "    print(f\"MLflow server started on http://{mlflow_host}:{mlflow_port}.\")\n",
    "else:\n",
    "    print(f\"MLflow server is already running on http://{mlflow_host}:{mlflow_port}.\")\n",
    "    # V√©rifier si un run est d√©j√† actif\n",
    "    if mlflow.active_run() is not None:\n",
    "        print(f\"Ending the active run with ID: {mlflow.active_run().info.run_id}\")\n",
    "        mlflow.end_run()\n",
    "\n",
    "# Configurer MLflow\n",
    "print(\"D√©marrer un run.\")\n",
    "mlflow.set_tracking_uri(cfg.model.mlflow.trackingUri)\n",
    "mlflow.set_experiment(cfg.model.mlflow.experiment.name)\n",
    "mlflow.start_run(run_name=cfg.model.mlflow.experiment.run.name)\n",
    "print(\"MLflow run started.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2 : Pr√©traitement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression des lignes vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des lignes vides\n",
    "df[df[\"tweet\"].isna() | (df[\"tweet\"] == \"\")]\n",
    "df = df[~(df['tweet'].isna() | (df['tweet'] == \"\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorisation termin√©e avec 1000 caract√©ristiques.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# V√©rifier le type de vectorizer configur√©\n",
    "if cfg.vectorizer._target_ == \"tfidfVectorizer\":\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=cfg.vectorizer.stopWords,\n",
    "        max_features=cfg.vectorizer.maxFeatures,\n",
    "        ngram_range=tuple(cfg.vectorizer.ngramRange)\n",
    "    )\n",
    "else:\n",
    "    raise KeyError(\"La configuration 'tfidVectorizer' est absente ou mal d√©finie dans 'vectorizer'.\")\n",
    "    \n",
    "# Appliquer fit_transform sur les tweets\n",
    "X = vectorizer.fit_transform(df['tweet'])\n",
    "\n",
    "print(f\"TF-IDF vectorisation termin√©e avec {X.shape[1]} caract√©ristiques.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3 : Entra√Ænement et Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D√©coupage des donn√©es (`dataSplitting`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donn√©es d√©coup√©es avec la strat√©gie 'trainValTest':\n",
      "Train: (957936, 1000), Validation: (319312, 1000), Test: (319313, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Encodage binaire de la cible\n",
    "y = df['id'].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "# V√©rifier la strat√©gie s√©lectionn√©e\n",
    "if cfg.strategy._target_ == \"trainValTest\":\n",
    "    # Charger les param√®tres de la strat√©gie\n",
    "    params = cfg.strategy\n",
    "    # D√©coupage Train/Test\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=params.testSize, random_state=params.randomSeed\n",
    "    )\n",
    "    # D√©coupage Train/Validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=params.validationSize, random_state=params.randomSeed\n",
    "    )\n",
    "    print(f\"Donn√©es d√©coup√©es avec la strat√©gie 'trainValTest':\")\n",
    "    print(f\"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "elif cfg.strategy._target_ == \"trainTest\":\n",
    "    # Charger les param√®tres de la strat√©gie\n",
    "    params = cfg.strategy\n",
    "    # D√©coupage Train/Test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=params.testSize, random_state=params.randomSeed\n",
    "    )\n",
    "    X_val, y_val = None, None  # Pas de validation pour cette strat√©gie\n",
    "    print(f\"Donn√©es d√©coup√©es avec la strat√©gie 'trainTest':\")\n",
    "    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "elif cfg.strategy._target_ == \"crossValidation\":\n",
    "    # Charger les param√®tres de la strat√©gie\n",
    "    params = cfg.strategy\n",
    "    kfold = KFold(n_splits=params.folds, shuffle=True, random_state=params.randomSeed)\n",
    "    folds = list(kfold.split(X, y))\n",
    "    print(f\"Donn√©es d√©coup√©es avec la strat√©gie 'crossValidation':\")\n",
    "    print(f\"Nombre de folds: {len(folds)}\")\n",
    "    # Exemple d'acc√®s au premier fold\n",
    "    train_idx, val_idx = folds[0]\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    X_test, y_test = None, None  # Pas de test explicite pour cette strat√©gie\n",
    "    print(f\"Premier fold - Train: {X_train.shape}, Validation: {X_val.shape}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Strat√©gie de d√©coupage des donn√©es '{cfg.strategy._target_}' non reconnue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation des Hyperparam√®tres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- M√©thodes d'optimisation :\n",
    "    - GridSearchCV :\n",
    "        - Explore toutes les combinaisons d‚Äôhyperparam√®tres d√©finis dans une grille.\n",
    "        - Utilise la validation crois√©e pour √©valuer chaque combinaison.\n",
    "        - Retourne :\n",
    "            - Les meilleurs hyperparam√®tres (best_params_).\n",
    "            - Le mod√®le entra√Æn√© avec ces hyperparam√®tres (best_estimator_).\n",
    "\n",
    "    - Optuna :\n",
    "        - Optimisation bay√©sienne avec espace de recherche dynamique.\n",
    "        - Fonction objectif :\n",
    "            - Entra√Æne un mod√®le avec des hyperparam√®tres sugg√©r√©s.\n",
    "            - √âvalue la performance via validation crois√©e (cross_val_score).\n",
    "        - Retourne :\n",
    "            - Les meilleurs hyperparam√®tres (best_params).\n",
    "            - Un mod√®le final configur√© avec ces param√®tres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 24 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters Found by GridSearch: {'C': 0.1, 'max_iter': 2000, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test Accuracy: 0.7389677213267233\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import optuna\n",
    "\n",
    "# Param√®tres pour l'optimisation\n",
    "if cfg.hyperparameterOptimization._target_ == \"gridSearch\":\n",
    "    param_grid = dict(cfg.hyperparameterOptimization.paramGrid)\n",
    "\n",
    "    # D√©finir les folds ou l'ensemble de validation\n",
    "    cv = (\n",
    "        cfg.validation.crossValidation.folds\n",
    "        if cfg.strategy._target_ == \"crossValidation\"\n",
    "        else [(list(range(X_train.shape[0])), list(range(X_val.shape[0])))]\n",
    "    )\n",
    "\n",
    "    # Cr√©ation de GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=LogisticRegression(),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        verbose=cfg.hyperparameterOptimization.verbosityLevel,\n",
    "        n_jobs=cfg.hyperparameterOptimization.parallelJobs,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # R√©cup√©ration des meilleurs param√®tres\n",
    "    best_params = grid_search.best_params_\n",
    "    model = grid_search.best_estimator_\n",
    "    print(f\"Best Parameters Found by GridSearch: {best_params}\")\n",
    "\n",
    "    # √âvaluation finale sur l'ensemble de test\n",
    "    if X_test is not None and y_test is not None:\n",
    "        test_score = model.score(X_test, y_test)\n",
    "        print(f\"Test Accuracy: {test_score}\")\n",
    "\n",
    "elif cfg.hyperparameterOptimization._target_ == \"optuna\":\n",
    "    def objective(trial):\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
    "        C = trial.suggest_float(\"C\", 0.1, 10, log=True)\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"liblinear\", \"saga\"])\n",
    "        max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "\n",
    "        model = LogisticRegression(penalty=penalty, C=C, solver=solver, max_iter=max_iter)\n",
    "\n",
    "        if cfg.strategy._target_ == \"crossValidation\":\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=cfg.validation.crossValidation.folds)\n",
    "            return scores.mean()\n",
    "        elif cfg.strategy._target_ == \"trainValTest\":\n",
    "            model.fit(X_train, y_train)\n",
    "            return model.score(X_val, y_val)\n",
    "\n",
    "    # Lancer l'optimisation avec Optuna\n",
    "    study = optuna.create_study(direction=cfg.hyperparameterOptimization.optuna.optimizationDirection)\n",
    "    study.optimize(objective, n_trials=cfg.hyperparameterOptimization.optuna.trialCount, timeout=cfg.hyperparameterOptimization.optuna.timeLimitSeconds)\n",
    "\n",
    "    # R√©cup√©ration des meilleurs param√®tres\n",
    "    best_params = study.best_params\n",
    "    model = LogisticRegression(**best_params)\n",
    "    print(f\"Best Parameters Found by Optuna: {best_params}\")\n",
    "\n",
    "    # √âvaluation finale sur l'ensemble de test\n",
    "    if X_test is not None and y_test is not None:\n",
    "        model.fit(X_train, y_train)\n",
    "        test_score = model.score(X_test, y_test)\n",
    "        print(f\"Test Accuracy: {test_score}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unsupported hyperparameter optimization method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 4 : √âvaluation Finale et Comparaison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### √âvaluation sur un ensemble de test ind√©pendant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### √âtape 4 : √âvaluation Finale ###\n",
      "Accuracy on Test Set: 0.7390\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.69      0.73    159233\n",
      "           1       0.72      0.78      0.75    160080\n",
      "\n",
      "    accuracy                           0.74    319313\n",
      "   macro avg       0.74      0.74      0.74    319313\n",
      "weighted avg       0.74      0.74      0.74    319313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# √âtape 4 : √âvaluation Finale et Comparaison\n",
    "if X_test is not None and y_test is not None:\n",
    "    print(\"\\n### √âtape 4 : √âvaluation Finale ###\")\n",
    "    \n",
    "    # √âvaluer le mod√®le sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "else:\n",
    "    print(\"\\n### √âtape 4 : √âvaluation Finale ###\")\n",
    "    print(\"Aucun ensemble de test ind√©pendant disponible pour √©valuation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enregistrement du run dans MLflow (m√©triques et artefacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enregistrement du run dans MLflow (m√©triques et artefacts)\n",
    "def log_run_metrics_and_artifacts(model, cfg, val_accuracy, val_f1, y_val=None, y_val_proba=None):\n",
    "    # Extraire les labels depuis la configuration Hydra\n",
    "    hydra_labels = {\n",
    "        \"data_split\": cfg.strategy._target_,\n",
    "        \"optimizer\": cfg.hyperparameterOptimization._target_,\n",
    "        \"validation\": cfg.validation.crossValidation.folds if cfg.strategy._target_ == \"crossValidation\" else \"N/A\",\n",
    "        \"experiment_name\": cfg.model.mlflow.experiment.name,\n",
    "        \"run_name\": cfg.model.mlflow.experiment.run.name,\n",
    "    }\n",
    "\n",
    "    # Log tags pour les labels Hydra\n",
    "    for key, value in hydra_labels.items():\n",
    "        mlflow.set_tag(key, value)\n",
    "\n",
    "    # Log des m√©triques dans MLflow\n",
    "    mlflow.log_metric(\"validation_accuracy\", val_accuracy)\n",
    "    mlflow.log_metric(\"validation_f1_score\", val_f1)\n",
    "\n",
    "    # Enregistrer le rapport de classification dans les logs\n",
    "    if y_val is not None:\n",
    "        val_classification_report = classification_report(y_val, model.predict(y_val))\n",
    "        with open(\"classification_report_val.txt\", \"w\") as f:\n",
    "            f.write(val_classification_report)\n",
    "        mlflow.log_artifact(\"classification_report_val.txt\")\n",
    "        os.remove(\"classification_report_val.txt\")\n",
    "\n",
    "    # Courbe ROC (si disponible)\n",
    "    if y_val_proba is not None:\n",
    "        val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "        mlflow.log_metric(\"validation_roc_auc\", val_roc_auc)\n",
    "\n",
    "        # G√©n√©rer la courbe ROC\n",
    "        fpr, tpr, _ = roc_curve(y_val, y_val_proba)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {val_roc_auc:.2f})\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid()\n",
    "\n",
    "        # Enregistrer la courbe ROC\n",
    "        roc_curve_path = \"roc_curve_validation.png\"\n",
    "        plt.savefig(roc_curve_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(roc_curve_path)\n",
    "        os.remove(roc_curve_path)\n",
    "\n",
    "    # Enregistrement du mod√®le dans MLflow\n",
    "    mlflow.log_param(\"model_name\", cfg.model.mlflow.model.name)\n",
    "    mlflow.sklearn.log_model(model, cfg.model.mlflow.model.name)\n",
    "\n",
    "    # Afficher les r√©sultats\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "    if y_val_proba is not None:\n",
    "        print(f\"Validation ROC AUC: {val_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run logistic_regression_run at: http://127.0.0.1:5000/#/experiments/277281536415448661/runs/5926d1d65e314b4db3e1f6deec1bbdf9\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/277281536415448661\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison avec d'autres mod√®les enregistr√©s dans le **Model Registry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLe meilleur mod√®le enregistr√© est \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) avec une validation accuracy de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Appel de la fonction pour comparer le mod√®le actuel\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m compare_with_registered_models(cfg, \u001b[43mval_accuracy\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Comparaison avec d'autres mod√®les enregistr√©s dans le Model Registry\n",
    "def compare_with_registered_models(cfg, val_accuracy):\n",
    "    client = MlflowClient(tracking_uri=cfg.model.mlflow.trackingUri)\n",
    "\n",
    "    # R√©cup√©rer tous les mod√®les enregistr√©s dans l'exp√©rience\n",
    "    registered_models = client.search_registered_models()\n",
    "    \n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    print(\"\\nComparaison avec les mod√®les enregistr√©s dans le Model Registry:\")\n",
    "    for model in registered_models:\n",
    "        model_name = model.name\n",
    "\n",
    "        # R√©cup√©rer les versions du mod√®le\n",
    "        for version in client.get_latest_versions(model_name):\n",
    "            if \"validation_accuracy\" in version.tags:\n",
    "                model_accuracy = float(version.tags[\"validation_accuracy\"])\n",
    "\n",
    "                print(f\"Mod√®le: {model_name}, Version: {version.version}, Validation Accuracy: {model_accuracy}\")\n",
    "\n",
    "                # Comparer les scores\n",
    "                if model_accuracy > best_accuracy:\n",
    "                    best_accuracy = model_accuracy\n",
    "                    best_model = (model_name, version.version)\n",
    "\n",
    "    print(\"\\nR√©sultats de la comparaison:\")\n",
    "    if val_accuracy > best_accuracy:\n",
    "        print(f\"Le mod√®le actuel est le meilleur avec une validation accuracy de {val_accuracy:.4f}.\")\n",
    "    else:\n",
    "        print(f\"Le meilleur mod√®le enregistr√© est '{best_model[0]}' (version {best_model[1]}) avec une validation accuracy de {best_accuracy:.4f}.\")\n",
    "\n",
    "# Appel de la fonction pour comparer le mod√®le actuel\n",
    "compare_with_registered_models(cfg, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 5 : Promotion et Validation en Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import onnxruntime as ort\n",
    "from azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "\n",
    "# Configuration Azure Application Insights\n",
    "exporter = AzureMonitorTraceExporter(connection_string=\"InstrumentationKey=85b10953-35ac-45bc-9192-6044192484fe\")\n",
    "tracer_provider = TracerProvider()\n",
    "span_processor = BatchSpanProcessor(exporter)\n",
    "tracer_provider.add_span_processor(span_processor)\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "# Charger le mod√®le ONNX\n",
    "onnx_session = ort.InferenceSession('./output/simple_model.onnx')\n",
    "\n",
    "def predict_sentiment(tweet):\n",
    "    \"\"\"Pr√©dire le sentiment d'un tweet avec le mod√®le ONNX.\"\"\"\n",
    "    inputs = {onnx_session.get_inputs()[0].name: [[tweet]]}  # Respect du format attendu\n",
    "    outputs = onnx_session.run(None, inputs)\n",
    "    predicted_label = outputs[0][0]  # 1 pour \"positif\", 0 pour \"n√©gatif\"\n",
    "    return \"Positif\" if predicted_label == 1 else \"N√©gatif\"\n",
    "\n",
    "def main():\n",
    "    print(\"=== Test de Sentiment ===\")\n",
    "    while True:\n",
    "        # Saisir un tweet\n",
    "        tweet = input(\"Entrez un tweet (ou tapez 'exit' pour quitter) : \")\n",
    "        if tweet.lower() == 'exit':\n",
    "            print(\"Au revoir !\")\n",
    "            break\n",
    "        \n",
    "        # Pr√©dire le sentiment\n",
    "        sentiment = predict_sentiment(tweet)\n",
    "        print(f\"Pr√©diction : {sentiment}\")\n",
    "        \n",
    "        # Demander validation utilisateur\n",
    "        validation = input(\"La pr√©diction est-elle correcte ? (oui/non) : \").strip().lower()\n",
    "        \n",
    "        if validation == \"non\":\n",
    "            print(\"Merci pour votre retour, une trace a √©t√© envoy√©e.\")\n",
    "            \n",
    "            # Envoyer une trace √† Application Insights\n",
    "            with tracer.start_as_current_span(\"Validation incorrecte\"):\n",
    "                span = trace.get_current_span()\n",
    "                span.set_attribute(\"tweet\", tweet)\n",
    "                span.set_attribute(\"pr√©diction\", sentiment)\n",
    "                span.set_attribute(\"validation\", \"Non\")\n",
    "        elif validation == \"oui\":\n",
    "            print(\"Merci pour votre validation.\")\n",
    "        else:\n",
    "            print(\"R√©ponse invalide. Veuillez entrer 'oui' ou 'non'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enregistrement dans le Model Registry "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "model_uri = f\"runs:/{run.info.run_id}/{cfg.mlflow.model.name}\"\n",
    "\n",
    "try:\n",
    "    # V√©rifier si le mod√®le existe dans le registre\n",
    "    client.get_registered_model(cfg.mlflow.model.name)\n",
    "except mlflow.exceptions.MlflowException:\n",
    "    # Cr√©er un mod√®le dans le registre s'il n'existe pas\n",
    "    client.create_registered_model(cfg.mlflow.model.name)\n",
    "\n",
    "# Cr√©er une nouvelle version du mod√®le dans le registre\n",
    "client.create_model_version(\n",
    "    name=cfg.mlflow.model.name,\n",
    "    source=model_uri,\n",
    "    run_id=run.info.run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "\n",
    "# Transitionner une version de mod√®le vers Production\n",
    "client.transition_model_version_stage(\n",
    "    name=cfg.mlflow.model.name,\n",
    "    version=1,  # La version √† transitionner\n",
    "    stage=\"Production\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p6-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
