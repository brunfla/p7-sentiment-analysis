Étape 1 : “Modèle sur mesure simple” 

    Livrable : 
        Notebook d’élaboration du modèle classique 
    Recommandations : 
        Réaliser un test sur le même sample du dataset que les modèles avancés, par exemple 16000 tweets, afin de comparer les résultats :
            Découper en 3 sets : Train, Validation, Test
            Le set Train est utilisé pour l’entraînement, le set de Validation pour mesurer les scores du modèle et comparer avec les autres modèles, le set de Test pour réaliser un contrôle final, notamment s’assurer d’un score similaire sur ce set qui n’est à aucun moment intervenu dans l’élaboration ou le choix du modèle
            Sur ce modèle classique, l’étudiant pourra réaliser une Cross-Validation sur le set Train. Par contre la comparaison avec les modèles avancés se fera sur le score du set Validation (score du set de Validation utilisé en deep learning pour élaborer les modèles)

 
Étape 2 : “Modèle avancé” – Élaboration des modèles

    Livrables : 
        Création de deux modèles de Deep Learning, dont au moins un avec un layer LSTM.
        Simulation selon deux techniques de pré-traitement (lemmatization, stemming) sur l’un des 2 modèles, afin de choisir la technique pour la suite des simulations.
        Simulation selon 2 approches de word embedding (parmi Word2VEc, Glove, FastText), entraînés avec le jeu de données ou pré-entraînés sur au moins un des 2 modèles de Deep Learning, afin de choisir l’embedding pour la suite des simulations.
        Création ensuite d’un modèle BERT, il y a 2 approches possibles : 
            Générer des features (sentence embedding) à partir d’un TFBertModel (Hugging Face) ou d’un d’un model via le Hub TensorFlow, puis ajouter une ou des couches de classification
            Utiliser directement un modèle Hugging Face de type TFBertForSequenceClassification
        En option tester USE (Universal Sentence Encoding) pour le feature engineering
        
    Problèmes et erreurs courants : 
        Temps de traitement et limitation de ressources en TensorFlow-Keras.
    Recommandations : 
        S’inspirer des exemples de modèles cités en ressources 
        Pour le modèle avec couche LSTM, l’utilisation d’un « Bidirectional (LSTM) » peut permettre de meilleurs résultats.
        Réaliser dans un premier temps des tests sur le même sample du dataset que l’approche classique (par exemple 16000 tweets), afin de comparer les résultats avec l’autre approche.
        Réaliser ensuite une simulation sur un jeu de données plus important sur le modèle choisi, en fonction des capacités machine à disposition.

 
Étape 3 : Suivi de la performance du modèle en production 

    Livrable : 
        Traces et alertes sur Azure Application Insight
        Blog partie “suivi de la performance du modèle en production” : présentation des traces et alertes sur Azure Application Insight, ainsi qu’une présentation d’une démarche qui pourrait être mise en oeuvre pour l’analyse de ces statistiques et l’amélioration du modèle dans le temps
    Recommandations : 
        L’étudiant crée un service Application Insight sur Azure
        Dans son interface de test de l’API (CF milestone suivant) il demande une validation à l’utilisateur de la pertinence de la prédiction, et envoie une “trace” au service Azure Application Insight en cas de non validation
        Il paramètre la gestion d’une alerte de son service Azure Application insight, pour déclencher une alerte, par exemple si 3 “traces” sont transmises sur un intervalle de 5 mn
        Il peut implémenter optionnellement la remontée également de “traces” de validation par l’utilisateur de la prédiction, ce qui permettra de calculer une accuracy en production, et mesurer une éventuelle dérive des prédictions. 
        Cette action, implémentée ou pas, fera partie du dispositif de “suivi de la performance du modèle en production” qui sera proposé et décrit dans le blog

 
Étape 4 : “Modèle avancé” – Mise en production 

    Livrables : 
        Pipeline de mise en production : code de l’API, géré en version en local avec GIT, commits et push du code sur Github, mise en production sur Cloud via les “actions” de Github : exécution d’un fichier yml de “build and deploy” de l’API, intégrant l’exécution automatique de tests unitaires lors du build
        Mise en production du modèle pour prédiction du sentiment (API - moteur d’inférence) : l’API renvoie un sentiment à réception d’un texte brut
        Interface en local (streamlit, notebook, application web) d’appel à l’API via son uri : interface de saisie du texte brut par l’utilisateur, appel de l’API, réception et affichage du sentiment, demande de validation à l’utilisateur de la pertinence de la prédiction, et envoie d’une trace au service Azure Application Insight en cas de non validation (optionnellement : également en cas de validation)
    Recommandations : 
        Les tests unitaires seront testés en local, puis exécutés automatiquement dans la partie “build” de déploiement via Github : pour cela il est nécessaire de rajouter dans le fichier .yml, dans la partie “deploy”, après l’installation des dépendances, 2 lignes “name: Run unit tests”, puis “run: python -m unittest” (ou pytest, si pytest est utilisé)
        Penser à sauvegarder l’ensemble du modèle Tensorflow via un « model.save([nom_répertoire]) » et non pas un fichier « .h5 »
        Si la solution Cloud gratuite choisie par l’étudiant ne permet pas de prendre en compte le modèle avancé Tensorflow-(composants, limite de taille, …), l’étudiant pourra déployer son modèle classique.  
        Une autre solution est de réduire la taille du modèle TensorFlow-Keras en testant la solution de conversion en un modèle “TensorFlow Lite” : https://www.tensorflow.org/lite/convert/index?hl=fr 
        Pour tester l’API de prédiction créée, une interface à l’aide de Streamlit est très rapide à mettre en œuvre (exécution en local).
        L’étudiant a le choix de la solution Cloud pour le déploiement de l’API, idéalement gratuite :
            Azure webapp (App Service)
                Choisir l’option de l’App Service Plan avec « Sku and size » égal idéalement à « Free F1 »  gratuit (code + packages < 1 Go) 
                Création sur le portail Azure d’une webapp, avec mise à jour du code automatiquement via une source Github
            PythonAnywhere
            Heroku, gratuit uniquement en mode “github student” (CF cadre bleu ci-dessus)
            Autres solutions au choix
            L’étudiant a la possibilité d’optimiser son déploiement en créant un conteneur Docker : https://openclassrooms.com/fr/courses/2035766-optimisez-votre-deploiement-en-creant-des-conteneurs-avec-docker


