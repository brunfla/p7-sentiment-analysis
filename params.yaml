# Prétraitement

# Découpage des données
split_data_train_val_test:
  input_file: data/input/training.1600000.processed.noemoticon.utf-8.csv
  output_dir: data/output/preprocessed/train_val_test/
  text_column: tweet
  label_column: id
  id_column: timestamp
  test_size: 0.005
  val_size: 0.07
  random_state: 42

plot_data_stats_train_val_test:
  input_dir: data/output/preprocessed/train_val_test
  input_files:
  - "x_train.csv"
  - "x_val.csv"
  - "x_test.csv"  
  output_dir: data/output/preprocessed/train_val_test/plots

clean_tweet_base:
  input_dir: data/output/preprocessed/train_val_test
  input_files:
  - "x_train.csv"
  - "x_val.csv"
  - "x_test.csv"
  output_dir: data/output/preprocessed/clean_tweet_base

clean_tweet_glove:
  input_dir: data/output/preprocessed/clean_tweet_base/ 
  input_files:
  - "x_train.csv"
  - "x_val.csv"
  - "x_test.csv"
  output_dir: data/output/preprocessed/clean_tweet_glove/
  glove_file: data/input/glove.twitter.27B.50d.txt
  glove_similarity_threshold: 0.8

plot_data_stats_clean_tweet_glove:
  input_dir: data/output/preprocessed/clean_tweet_glove
  input_files:
  - "x_train.csv"
  - "x_val.csv"
  - "x_test.csv"  
  output_dir: data/output/preprocessed/clean_tweet_glove/plots

# Prétraitement avec TF-IDF
batch_apply_tfidf_vectorizer:
  input_dir: data/output/preprocessed/clean_tweet_glove
  input_files:
  - x_train.csv
  - x_val.csv
  - x_test.csv
  - y_train.csv
  - y_val.csv
  - y_test.csv
  output_dir: data/output/preprocessed/tfidf_vectorizer
  max_length: 10000

batch_prepare_glove_vectors:
  glove_file: "data/input/glove.twitter.27B.50d.txt"
  input_dir: "data/output/preprocessed/clean_tweet_glove"
  input_files:
    - "x_train.csv"
    - "x_val.csv"
    - "x_test.csv"
    - "y_train.csv"
    - "y_val.csv"
    - "y_test.csv"
  output_dir: "data/output/preprocessed/glove_vectors"
  max_length: 10000
  embedding_dim: 50

train_lstm_bidirectional:
  input_train_vec: data/output/preprocessed/glove_vectors/x_train_glove.pkl
  input_train_labels: data/output/preprocessed/glove_vectors/y_train.csv
  input_val_vec: data/output/preprocessed/glove_vectors/x_val_glove.pkl
  input_val_labels: data/output/preprocessed/glove_vectors/y_val.csv
  glove_file: data/input/glove.twitter.27B.50d.txt
  output_dir: data/output/trained_models/lstm_bidirectional
  model_params:
    max_length: 15
    lstm_units: [128, 64]
    dropout_rate: 0.3
    dense_units: 32
    batch_size: 160
    epochs: 5
  training_params:
    earlyStopping:
      enabled: true
      monitor: val_loss
      patience: 3
      mode: min
    learningRateScheduler:
      enabled: true
      monitor: val_loss
      factor: 0.5
      patience: 2
      min_lr: 1e-5
    thresholdStop:
      enabled: false  # Désactiver si pas implémenté dans le script
      threshold: 0.6
      metric: accuracy
      patience_batches: 10
  glove_params:
    embedding_dim: 50
    trainable: false
  mlflow:
    trackingUri: http://mlflow.local
    experiment:
      name: sentiment-analysis
      run:
        name: lstm_bidirectional
        description: Training with bidirectional LSTM and GloVe embeddings
        tags:
          model: lstm_bidirectional
          dataset: glove_vectors
  data_params:
    padding: post
    truncating: post

test_lstm_bidirectional_with_glove:
  input_test_vec: data/output/preprocessed/glove_vectors/x_test_glove.pkl
  input_test_labels: data/output/preprocessed/glove_vectors/y_test.csv 
  output_dir: data/output/trained_models/lstm_bidirectional
  model_file: bilstm_model.h5
  threshold: 0.5
  run_id_file: data/output/trained_models/lstm_bidirectional/mlflow_id.json
  mlflow:
    trackingUri: "http://mlflow.local"
  metrics:
    - accuracy
    - precision
    - recall
    - f1
    - roc_auc
    - pr_auc

# Entraînement des modèles
train_logistic_tfidf:
  input_train_vec: data/output/preprocessed/tfidf_vectorizer/x_train_vec.pkl
  input_train_labels: data/output/preprocessed/tfidf_vectorizer/y_train.csv
  input_val_vec: data/output/preprocessed/tfidf_vectorizer/x_val_vec.pkl
  input_val_labels: data/output/preprocessed/tfidf_vectorizer/y_val.csv
  output_dir: data/output/trained_models/logistic_sgd
  model_type: logistic_sgd
  tuning:
    max_iter: 1000
    tol: 0.001
  chunk_size: 10000
  mlflow:
    trackingUri: "http://mlflow.local"
    experiment:
      name: "sentiment-analysis"
      run:
        name: "logistic_regression_tfidf"
        description: "Training Logistic Regression with TF-IDF"
        min_accuracy: 0.5  # Seuil minimal d'accuracy pour enregistrer le modèle
        tags:
          modelType: "logistic_sgd"
          dataset: "tfidf_vectorizer"

test_logistic_tfidf:
  input_test_vec: data/output/preprocessed/tfidf_vectorizer/x_test_vec.pkl
  input_test_labels: data/output/preprocessed/tfidf_vectorizer/y_test.csv
  model_run_id_file: data/output/trained_models/logistic_sgd/mlflow_id.json
  output_metrics_json: data/output/trained_models/logistic_sgd/test_metrics.json
  plot_dir: data/output/trained_models/logistic_sgd/plots  # Nouveau répertoire pour les plots
  threshold: 0.5
  mlflow:
    trackingUri: "http://mlflow.local"

train_distilbert_ktrain:
  # Modèle pré-entraîné
  pretrained_model: distilbert-base-uncased
  # Fichiers de données tokenisées (format .pkl)
  input_dir: data/output/preprocessed/clean_tweet_glove/
  input_files:
  - x_train.csv
  - y_train.csv 
  - x_val.csv
  - y_val.csv 
  # Répertoire pour les sorties du modèle
  output_dir: data/output/trained_models/distilbert-base-uncased
  # Paramètres du modèle
  model_params:
    max_length: 25  # Longueur maximale des séquences
    epochs: 4      # Nombre d'époques d'entraînement
    batch_size: 32   # Taille du batch
  # Paramètres d'entraînement
  training_params:
    learning_rate: 0.000005  # Taux d'apprentissage
  # Configuration MLflow pour le suivi des expériences
  mlflow:
    trackingUri: http://mlflow.local  # URI du serveur MLflow
    experiment:
      name: sentiment-analysis       # Nom de l'expérience
      run:
        name: distilbert-base-uncased-ktrain  # Nom du run
        description: Training distilbert-base-uncased with Ktrain
        tags:                        # Tags pour le suivi
          model: distilbert-base-uncased
          dataset: clean_tweet_glove

test_model_single_tweet:
  input_dir: data/output/preprocessed/clean_tweet_glove/
  input_files:
  - x_test.csv
  - y_test.csv
  output_dir: data/output/trained_models/distilbert-base-uncased
  threshold: 0.5
  model_run_id_file: data/output/trained_models/distilbert-base-uncased/mlflow_id.json
  mlflow_tracking_uri: "http://mlflow.local"
  model_params:
    max_length: 20  # Doit correspondre à celui utilisé dans preprocess_token_bert
    batch_size: 160  # Réduire à 16 si problèmes mémoire
    epochs: 1       # Early stopping interrompt si besoin
  metrics:
    - accuracy
    - precision
    - recall
    - f1
    - roc_auc
    - pr_auc

build_and_test_api:
  docker_file: Dockerfile
  image_name: distilbert-base-uncased
  tag_name: latest
  input_test_features: data/output/preprocessed/clean_tweet_glove/x_test.csv 
  input_test_labels: data/output/preprocessed/clean_tweet_glove/y_test.csv
  test_script: src/deploy/test_api.py
  registry_name: bflament # dockerhub) 
